[{"title":"3231note","date":"2023-04-25T14:32:58.000Z","url":"/2023/04/26/3231note/","tags":[["COMP3231","/tags/COMP3231/"]],"categories":[["notes","/categories/notes/"]],"content":"UNSW COMP3231 23T1 notes Lec 02 Processes and ThreadsProcesses:• Also called a task or job • Memory image of an individual program • “Owner” of resources allocated for program execution • Encompasses one or more threads Threads:• Unit of execution 执行命令的单位 • Can be traced • list the sequence of instructions that execute • Belongs to a process • Executes within it. One Process may contain one or more threads. Process TerminationConditions which terminate processes 1. Normal exit (voluntary) 2. Error exit (voluntary) 3. Fatal error (involuntary) 4. Killed by another process (involuntary) Process and Thread StatesRunning Blocked Ready Running → Ready• Voluntary Yield() • End of timeslice Running → Blocked• Waiting for input • File, network, • Waiting for a timer (alarm signal) • Waiting for a resource to become available SchedularSometime called as dispatcher, it will choose a ready process to run. BUT: It is inefficient to search through all processes e.g: Ready Queue &amp; Blocked Queue Thread Model Per process items Per thread items Address spaceGlobal variablesOpen filesChild processesPending alarmsSignals and signal handlersAccounting information Program counterRegistersStack (Each thread has its own stack)State Local variables are per thread (allocated on the stack) Global variables are shared between all threads (Allocated in the data section) Concurrency control is an issue. Dynamically allocated memory can be global or local -&gt; Program defined Thread Usage Model 模型 Characteristics 特性 Threads Parallelism. Blocking system calls Single-threaded Process No parallelism, blocking system calls Finite-state Machine Parallelism, nonblocking system calls, interrupts Summary in Threads Simpler to program than a state machine Less resources associated with threads than multiple complete processes Cheaper to create and destroy Share resources (especially memory) between them Performance: Threads waiting for IO can be overlapped with computing threads If all threads are compute bound(被运算速度限制), then there is no performance improvement (on uniprocessor) Threads can take advantage of the parallelism available on machines with more than one CPU (multiprocessor) Lec 03 Concurrency and SynchronisationConcurrency Example:Trying to modify a global variable in two threads The reason for why there is in-kernel concurrency for single-threaded processes: Multi-tasking Interrupt handling Device drivers System calls Resource sharing Critical Region and how to identifyA critical region is a region of code where shared resources are accessed. We can control access to the shared resource by controlling access to the code that accesses the resource. Uncoordinated entry to the critical region results in a race condition (Concurrency Occurs) HOW TO IDENTIFY: Critical regions are the regions of code that access a shared resource, and correctness relies on the shared resource not being concurrently modified by another thread&#x2F;process&#x2F;entity.\\ HOW TO PREVENT: Critical Regions SolutionsConditions required of any solution to the critical region problem: Mutual exclusion: No two processes&#x2F;threads in the critical region at the same time No assumptions made about speeds or numbers of CPUs No process running outside its critical region may block another process No process waited forever to enter its critical region (Bounded) Mutual exclusion by taking turnsWorks for solving concurrency issues: strict alternation -&gt; each process takes turns Cons (缺点) Busy waiting Process must wait its turn even while the other process is doing something else In a multiple processes condition, must wait for everyone to have a turn Does not guarantee progress if a process no longer needs a turn Poor solution when processes require the critical section at different rates Mutual Exclusion by disabling interruptsHow it works: Before entering a critical region, disable interrupts. After leaving the critical region, enable interrupts. Cons: Only available in the kernel Delays everybody else, even with no contention Slows the interrupt response time Does not work on a multiprocessor Hardware Support for mutual exclusionTest and Set instruction •Can be used to implement lock variables correctly •Load the value of the lock •If lock &#x3D;&#x3D; 0 •Set the lock to 1 •Return the result 0 -&gt; we acquire the lock •if lock &#x3D;&#x3D; 1 • return 1 -&gt; another thread&#x2F;process has the lock •Hardware guarantees that the instruction executes atomically -&gt; not be interrupt Pros: Simple (easy to verify work or not) Available at user level Work with any number of processors To implement any number of lock variables Cons: Busy waits (also named as a spin lock) Consumes CPU Starvation is possible when a process leaves its critical section and more than one process is waiting. Tackling the busy-wait problem:Sleep&#x2F;Wakeup When the process is waiting for an event, it calls sleep to block, instead of busy waiting. The event happens, the event generator (another process) calls wakeup to unblock the sleeping process. Waking a ready&#x2F;running process has no effect. Producer-Consumer Problem(bounded buffer problem)Description: producer produces data items and store items in a buffer. Consumer takes the items out of the buffer and consumes them. Issues:Producer Should sleep when the buffer is full. And wakeup when there is empty space in the buffer. The consumer can call wakeup when it consumes the first entry of the full buffer. Consumer Should sleep when the buffer is empty. And wakeup when there are items available. Producer can call wakeup when it adds the first item to the buffer. SemaphoresTwo primitives that are more powerful than simple sleep and wakeup alone. • P(): proberen, from Dutch to test. • V(): verhogen, from Dutch to increment. • Also called wait &amp; signal, down &amp; up. How it works: If a resource is not available, the corresponding semaphore blocks any process waiting for the resource. Blocked processes are put into a process queue maintained by the semaphore (avoids busy waiting!) When a process releases a resource, it signals this by means of the semaphore. Signaling resumes a blocked process if there is any. Wait (P) and signal (V) operations cannot be interrupted. Complex coordination can be implemented by multiple semaphores. The initial count determines how many waits can progress before blocking and requiring a signal. Producer Consumer problem with semaphores: Summarising SemaphoresSemaphores can be used to solve a variety of concurrency problems. However, programming with then can be error-prone: E.g. must signal for every wait To many or too few signals or waits, or signals and waits in the wrong order, can have bad results. MonitorA higher level synchronisation primitive. Programming language construct IDEA: A set of procedures&#x2F;variables&#x2F;data types are grouped in a special module:monitor. Variables and data types can only be accessed from within the monitor. Only one process&#x2F;thread can be in the monitor at any one time. Mutual exclusion is implemented by the complier. When a thread calls a monitor procedure that has a thread already inside, it will be in the entry queue and will sleep until the current thread exits the monitor. Condition VariableTo allow a process to wait within the monitor, we need to declare a condition variable. Condition variable can only be used with the operations wait and signal. x.wait() Means that the process invoking this operation is suspended until another process invokes. Another thread can enter the monitor while original is suspended. x.signal() this operation resumes only one suspended process. If no process is suspended, then this operation has no effect. How to achieve producer-consumer problem with monitors is in P50 lec03. Synchronisation Primitives in OS&#x2F;161Locks Semaphores Wait (P) &#x2F; signal (V) Condition Variables Note: All three variants must hold the lock passed in. Condition Variables and Bounded Buffers cv_wait() will release the lock before block the process Producer-consumer solution with condition variable Dining Philosophers哲学家就餐问题是一个经典的计算机科学同步问题，由Edsger Dijkstra在1965年提出。问题描述了五位哲学家围坐在圆桌旁的情景，他们在思考问题和吃饭之间交替。圆桌中间有一碗意面，每两位哲学家之间有一根筷子。每位哲学家需要两根筷子才能吃饭。哲学家们不能交谈，只能思考或吃饭。 问题在于设计一个协议，使得每位哲学家都能够在需要时使用两根筷子吃饭，而不会出现死锁（即所有哲学家都在等待筷子，无法继续吃饭）或者饥饿（即某位哲学家长时间无法获得筷子）的情况。 以下是一个可能的解决方案： 我们可以为每根筷子分配一个编号（例如，1到5），并为每位哲学家分配一个编号。每位哲学家在需要吃饭时，先尝试拿起编号较低的筷子，然后尝试拿起编号较高的筷子。当哲学家同时拥有两根筷子时，他们可以开始吃饭。吃完饭后，他们将筷子放回原位，然后继续思考。 这种方法可以避免死锁，因为至少有一位哲学家（具有最低编号筷子的哲学家）可以拿起两根筷子并开始吃饭。其他哲学家则需要等待筷子被放回。虽然这种方法可能导致某些哲学家等待时间较长，但可以确保所有哲学家都有机会吃饭，避免饥饿现象。 Reader and Writer Problem读者-写者问题是一个经典的计算机科学并发控制问题，用于描述多个进程在访问共享资源时需要进行同步的场景。在这个问题中，有一些读者进程和写者进程。读者进程只读取共享资源，而写者进程可以修改共享资源。问题的挑战在于设计一个同步协议，以允许多个读者进程同时访问共享资源，但在写者进程访问资源时，确保其他进程（读者和写者）无法访问资源。 以下是一个可能的解决方案： 我们可以使用互斥量（mutex）和信号量（semaphore）来实现这个协议。在这个解决方案中，我们需要一个互斥量来保护对共享资源的访问计数器，以及一个信号量来控制对共享资源的访问。 读者进程： 请求互斥量以修改访问计数器。 将访问计数器加1。如果这是第一个读者进程，请求信号量以阻止写者进程访问共享资源。 释放互斥量。 访问共享资源。 请求互斥量以修改访问计数器。 将访问计数器减1。如果这是最后一个读者进程，释放信号量以允许写者进程访问共享资源。 释放互斥量。 写者进程： 请求信号量以阻止其他进程访问共享资源。 访问共享资源。 释放信号量以允许其他进程访问共享资源。 这个解决方案允许多个读者进程同时访问共享资源，但当有写者进程需要访问资源时，它们将等待所有当前的读者进程完成。同样，在写者进程访问共享资源时，其他读者和写者进程将被阻止。这种方法可以确保在写者进程访问资源时，没有其他进程可以访问共享资源。 Lec04 DeadLockDeadLock &amp; ResourcesDeadlocks occurs when Processes are granted exclusive access to devices&#x2F;locks&#x2F;tables.. We refer to these entities generally as resources. Deadlock: definition A set of processes is deadlocked if each process in the set is waiting for an event that only another process in the set can cause. In the deadlock situation, no process can Run Release resources Be awakened Four conditions for deadlock Mutual exclusion condition Each resource assigned to 1 process or is available Hold and wait condition Process holding resources can request additional No preemption condition Previously granted resources cannot be forcibly taken away Circular wait condition Must be a circular chain of 2 or more processes Each is waiting for resource held by next member of the chain Deadlock avoidance Just ignore the problem altogether Prevention 预防 Negating on of the four necessary conditions Detection and recovery Dynamic avoidance Careful resource allocation Method 1: Ostrich AlgorithmPretend there is no problem Reasonable if Deadlocks occur very rarely Cost of prevention is high UNIX and Windows takes this approach for some of the more complex resource relationships they manage It’s a trade off between Convenience (engineering approach) Correctness (mathematical approach) Method 2: Deadlock preventionResource allocation rules prevent deadlock by prevent one of the four conditions required for deadlock from occurring: Mutual exclusion Hold and wait No preemption Circular Wait Attacking the Mutual Exclusion Condition (Not feasible)Not feasible in general Some devices&#x2F;resources are intrinsically(本质上) not sharable. Attacking the hold and wait condition (request resources initially)Require processes to request resources before starting A process never has to wait for what it needs Issues: May not know required resources at start of run Which means not always possible Also ties up resources other processes could be using Variations: Process must give up all resources if it would block holding a resource Then request all immediately need Prone(倾向于) to livelock Attacking the No Preemption condition(无优先权条件)(take resources away)Not a viable option Attacking the circular wait condition (Order resources)Numerically ordered resources (Resources ordering is a common technique) Method 3: Detection and recoveryNeed a method to determine if a system is deadlocked. Assuming deadlocked is detected, we need a method of recovery to restore progress to the system. Strategy:Note the resource ownership and requests A cycle can be found within the graph, which denotes a deadlock Detection with Multiple Resources of Each Type Sum of current resource allocation + resources available &#x3D; resources that exist Algorithm Look for an unmarked process Pi, for which the i-th row of R is less than or equal to A. If found, add the i-th row of C to A, and mark Pi, then go back to step 1. If no such process exists, terminate. The remaining processes are deadlocked. Recovery from DeadlockRecovery through preemption • take a resource from some other process • depends on nature of the resource Recovery through rollback • checkpoint a process periodically • use this saved state • restart the process if it is found deadlocked • No guarantee is won’t deadlock again Recovery through killing processes • crudest but simplest way to break a deadlock • kill one of the processes in the deadlock cycle • the other processes get its resources • choose process that can be rerun from the beginning Method 4: Deadlock AvoidanceOnly enough information is available in advance, we can avoid a deadlock. Maximum number of each resource required Safe and Unsafe statesA state is safe if • The system is not deadlocked • There exists a scheduling order that results in every process running to completion, even if they all request their maximum resources immediately Unsafe states are not necessarily deadlocked • With a lucky sequence, all processes may complete • However, we cannot guarantee that they will complete (not deadlock) • Safe states guarantee we will eventually complete all processes •Deadlock avoidance algorithm • Only grant requests that result in safe states LiveLockLivelocked processes are not blocked, change state regularly, but never make progress Bankers AlgorithmModelled on a Banker with Customers • The banker has a limited amount of money to loan customers • Limited number of resources • Each customer can borrow money up to the customer’s credit limit • Maximum number of resources required Basic Idea • Keep the bank in a safe state • So all customers are happy even if they all request to borrow up to their credit limit at the same time. • Customers wishing to borrow such that the bank would enter an unsafe state must wait until somebody else repays their loan such that the the transaction becomes safe StarvationA process never receives the resource it is waiting for, despite the resource (repeatedly) becoming free, the resource is always allocated to another waiting process One Solution: First come, first server Lec05 Processes and Threads ImplementationMIPS RegistersUser-mode accessible registers • 32 general purpose registers • r0 hardwired to zero • r31 the link register for jump-and-link (JAL) instruction HI&#x2F;LO • 2 * 32-bits for multiply and divide PC • Not directly visible • Modified implicitly by jump and branch instructions Branching and JumpingBranching and jumping have a branch delay slot • The instruction following a branch or jump is always executed prior to destination of jump ProcessMemory allocationMinimally consist of three segments • Text • contains the code (instructions) • Data • Global variables • Stack • Activation records of procedure&#x2F;function&#x2F;method • Local variables Note: • data can dynamically grow up • E.g., malloc()-ing • The stack can dynamically grow down • E.g., increasing function call depth or recursion P23 lec05 Mode distinguishUser-mode • Processes (programs) scheduled by the kernel • Isolated from each other • No concurrency issues between each other System-calls transition into and return from the kernel Kernel-mode • Nearly all activities still associated with a process • Kernel memory shared between all processes • Concurrency issues exist between processes concurrently executing in a system call User-level Threads Implementation at user-level • User-level Thread Control Block (TCB), ready queue, blocked queue, and dispatcher • Kernel has no knowledge of the threads (it only sees a single process) • If a thread blocks waiting for a resource held by another thread inside the same process, its state is saved and the dispatcher switches to another ready thread • Thread management (create, exit, yield, wait) are implemented in a runtime support library Pros• Thread management and switching at user level is much faster than doing it in kernel level • No need to trap (take syscall exception) into kernel and back to switch • Dispatcher algorithm can be tuned to the application • E.g. use priorities • Can be implemented on any OS (thread or non-thread aware) • Can easily support massive numbers of threads on a per-application basis • Use normal application virtual memory • Kernel memory more constrained. Difficult to efficiently support wildly differing numbers of threads for different applications Cons• Threads have to yield() manually (no timer interrupt delivery to user level) • Co-operative multithreading • A single poorly design&#x2F;implemented thread can monopolise the available CPU time • There are work-arounds (e.g. a timer signal per second to enable pre-emptive multithreading), they are course grain and a kludge. • Does not take advantage of multiple CPUs (in reality, we still have a single threaded process as far as the kernel is concerned) • If a thread makes a blocking system call (or takes a page fault), the process (and all the internal threads) blocks • Can’t overlap I&#x2F;O with computation Kernel-provided ThreadsP38 lec05 Threads are implemented by the kernel • TCBs (thread control block) are stored in the kernel • A subset of information in a traditional PCB (process control block) • The subset related to execution context • TCBs have a PCB associated with them • Resources associated with the group of threads (the process) • Thread management calls are implemented as system calls • E.g. create, wait, exit Cons• Thread creation and destruction, and blocking and unblocking threads requires kernel entry and exit. • More expensive than user-level equivalent Pros• Preemptive multithreading • Parallelism • Can overlap blocking I&#x2F;O with computation • Can take advantage of a multiprocessor Context SwitchContext Switch is what the lowest level of OS does when an interrupt occurs. A context switch can refer to • A switch between threads • Involving saving and restoring of state associated with a thread • A switch between processes • Involving the above, plus extra state associated with a process. • E.g. memory maps Context Switch OccurrenceA switch between process&#x2F;threads can happen any time the OS is invoked • On a system call • Mandatory if system call blocks or on exit(); • On an exception • Mandatory if offender is killed • On an interrupt • Triggering a dispatch is the main purpose of the timer interrupt A thread switch can happen between any two instructions Note instructions do not equal program statements Context Switch Addition Context switch must be transparent for processes&#x2F;threads • When dispatched again, process&#x2F;thread should not notice that something else was running in the meantime (except for elapsed time) OS must save all state that affects the thread • This state is called the process&#x2F;thread context • Switching between process&#x2F;threads consequently results in a context switch."},{"title":"test for categories","date":"2023-03-21T08:13:40.000Z","url":"/2023/03/21/test-for-cat/","tags":[["test","/tags/test/"]],"categories":[["Demo","/categories/Demo/"]],"content":"This is a blog for testing! 仅供测试左下角有音乐哦玩得愉快 JUST A DEMO! test for alerts喵呼呼喵呼呼o(&#x3D;•ェ•&#x3D;)m 成功成功啦o(￣▽￣)ブ 危险有危险Σ(っ °Д °;)っ 消息有消息(・∀・(・∀・(・∀・*) 当心当心哦≧ ﹏ ≦ test for folder 这是一个展开了的folder 什么都没有捏 这是一个关闭的folder 记得打开音乐哦！ *你上当了捏* test for blur这是一句被模糊的语句 test for image insert尝试点击一下吧👇 test for playing youtube 没想到吧是双重诈骗 thats all！"},{"title":"hello-world","date":"2023-03-20T06:04:12.000Z","url":"/2023/03/20/hello-world/","categories":[["Demo","/categories/Demo/"]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"}]