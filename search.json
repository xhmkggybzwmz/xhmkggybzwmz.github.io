[{"title":"DESN2000 COMP note","date":"2023-06-08T15:56:21.000Z","url":"/2023/06/09/DESN2000-COMP-note/","tags":[["DESN2000","/tags/DESN2000/"]],"categories":[["notes","/categories/notes/"]],"content":"UNSW DESN2000 COMP Stream note keep updating "},{"title":"2511 Design note","date":"2023-06-08T15:56:04.000Z","url":"/2023/06/09/2511-Design-note/","tags":[["COMP2511","/tags/COMP2511/"]],"categories":[["notes","/categories/notes/"]],"content":"UNSW 2511 23T2 note (Design part) keep updating 使用UML的Domain Modelling领域模型 Domain models Domain models 被用来直观地表示重要的 领域概念(Domain concepts) 和他们之间的 关系 Domain models 有助于澄清(clarify)和沟通(communicate)重要的特定领域概念， 并在需求收集和设计阶段中使用 领域建模 (domain modelling) 是将相关领域概念表达为一个领域模型 领域模型也通常被称为概念模型(conceptual models)或领域对象模型(domain object models) 需求分析 &amp; 领域建模 Requirements Analysis vs Domain modelling 需求分析决定了外部行为 (external behaviour) 未来的系统特点是什么，谁需要这些特点 (actors) 领域建模决定了 内部行为 (internal behaviour) 未来系统的元素如何互动以产生外部行为 需求分析和领域建模是 相互依赖(mutually dependent) 的，领域建模支持需求的澄清(clarification of reqs)，而需求分析有助于建立模型 What is a domain Domian: 与解决问题有关的知识领域 Domain expert 领域专家：该领域的专家例如：在蛋糕装饰领域， 蛋糕装饰师是领域专家 无处不在的语言 Ubiquitous Language 我们设计中的事物必须代表领域专家心智模型中的真正事物 例如，如果领域专家称某物为 “订单order”， 则在我们的领域模型和我们的最终实现中，应该有一个叫做 订单order 的东西 同样地， 我们的领域模型不应该包含 OrderHelper&#x2F;OrderManager等 技术细节不构成领域模型的一部分，因为其不属于设计 名词&#x2F;动词分析 Noun&#x2F;Verb analysis 通过寻找需求中的名词和东西来找到领域的泛在语言 名词是领域模型中可能存在的实体，动词是可能存在的行为 样例分析 图例： 名词 动词 游客的日程安排至少涉及一个甚至几个城市 酒店有各种不同等级的房间：标准间和高级间 旅游团是以标准价或高级价预订的，表明了酒店房间的等级。 在旅游的每个城市，游客都会被预订到所选等级的酒店房间。 游客预订的每个房间都有一个到达日期和一个离开日期。 酒店的名称（例如：墨尔本凯悦酒店）和房间的编号都会被识别。 游客可以在他们的旅程上预订、取消或更新日程。 Tourists have schedules that involve at least one and possibly several cities Hotels have a variety of rooms of different grades: standard and premium Tours are booked at either a standard or premium rate, indicating the grade of hotel room In each city of their tour, a tourist is booked into a hotel room of the chosen grade Each room booking made by a tourist has an arrival date and a departure date Hotels are identified by a name (e.g. Melbourne Hyatt) and rooms by a number Tourists may book, cancel or update schedules in their tour UML 类图 Dependency 依赖Dependency是最松散的关系形式， 即一个类在某种程度上依赖于另一个类 Association 关联一个类以某种方式“使用” 一个类， 当没有箭头指向时，表达的意图为不清楚依赖关系发生在什么方向上 Directed Association 指向关联通过指出哪个类对另一个类的了解来完善关联性 Aggregation 聚合一个类包含另一个类(例如： 课程包含学生)， 注意： 钻石型标记应该与包含的类放在一起 构成 Composition类似于聚合，但是包含的类与被包含的类是一体的，被包含的类不能处于容器之外 (e.g. 椅子和椅子腿) UML Diagram Types 在UML中表示类 Representing classes in UML 在UML中表示关联关系 Representing Association in UML 关联(Association) 可以建议一个”has-a“关系的模型-&gt; 一个类包含另一个类关联可进一步被定义为： 聚合关系 Aggregation (空心钻石符号)： 被包含的项是一个集合中的元素，但是其也可以单独存在， 例如大学中的讲师&#x2F;学生 构成关系 Composition (实心钻石符号)： 被包含的项是包含项目的一个组成部分， 例如椅子和椅子腿 注： 1..* ： 代表可以存在一个或多个 属性与类 Attribute vs Classes 最常见的疑惑：应该是属性还是一个类？ 在创建domain model时，经常需要决定是把某个东西作为 属性Attribute 还是 概念类Conceptual Class 表示 如果一个概念不能用数字或字符串表示，则它很可能是一个类 例如： lab mark 可以用数字表示，所以应该把他列为 AttributeStudent 不可以用数字&#x2F;字符串表示，所以应该把它列为 class 契约式设计 Design by Contract防御性编程 vs 契约式设计 Defensive Programming vs Design by contract防御性编程 Defensive programming试图解决不可预见的情况， 以确保软件元素的持续功能。 例如，尽管有意外的输入或用户行为，但防御性编程仍够能使软件以可预测的方式运行 通常需要用于 高可用性(high availability)&#x2F;安全(safety)&#x2F; 保障(security) 的地方 会导致多余的检查(redundant checks)(客户端和供应端都可能进行检查)，让 软件维护(complex software) 更为复杂 因为没有明确的责任划分 (no clear demaration of responsibilities), 所以很难定位错误位置 契约式设计 Design by contract在设计时，责任被明确地划分clear assigned给不同的软件元素并被明确记录下来，并在开发过程中使用单元测试(unit testing)&#x2F;语言支持来执行 职责的明确划分有助于防止多余的检查，从而使代码更简单&#x2F;易于维护 如果不满足所要求的条件，则程序会崩溃. 可能不适用于高可用性应用(not be suitable for high availability applications) 契约式设计 Design by contract (DbC) 契约设计(DbC) 起源于形式化规范(formal specification), 形式化验证 (formal verification)和 Hoare逻辑方面的工作 E.G.: 每个软件元素都应该定义一个规范(或契约), 以管理它与其他软件组件的互动(interacion), 一个契约应该解决以下三个问题： 前提条件 Pre-condition 契约期望什么 如果前提条件是真的，就可以避免处理前提条件之外的情况 例如：预期的参数值为 (mark &gt;&#x3D; 0 and mark &lt;&#x3D; 100) 后置条件 Post-condition 契约保证什么 只要满足前提条件，就能保证有返回值 例如：正确的返回值代表一个分数 不变条件 Invariant 契约保持什么 在执行前和执行后(例如方法的执行)，一些值需要满足约束条件 例如： 分数的值保持在0-100之间 契约(包括pre-condition&#x2F;post-condition&#x2F;invariant)应该是： 声明性的(declarative)，不能包括实施的细节 尽可能做到 精确(precise)&#x2F;正式(formal)和可验证(verifiable) 契约式设计的优点 Benefits of Design by Contract (DbC) 不需要为不满足前提条件的条件做错误检查 防止了多余的验证任务 鉴于已经满足前提条件，客户可以期待指定的后置条件得到实现 职责划分明确，有助于定位错误，简易代码维护 有助于 更清洁&#x2F;快速(cleaner and faster) 的开发 契约式设计的实施问题 Implementation issues 有一些编程语言(例如 Eifferl) 提供对于契约式设计的本地支持(native support) JAVA本地并不支持契约式设计，但是有库可以支持契约式设计 在语言没有本地支持的情况下，单元测试(unit tests) 被用来测试契约是否满足(即 前提条件 pre-condition&#x2F;后置条件 post-condition&#x2F;不变量 invariants) 通常情况下， 前提条件&#x2F;后置条件&#x2F;不变量 都 包含在文件&#x2F;注释 (documentation) 中 如上所述，契约应该是： 声明性的，不能包括实施细节尽可能做到 精确percise&#x2F;正式formal&#x2F;可检查 verifiable Java中的契约式设计样例： 前提条件 Pre-Conditions 前提条件是一个前提条件或一个谓语，它必须在执行代码的某个部分之前始终为真 如果前提条件被违反了，则这部分代码的效果就会变得 **不确定(undefined)**，从而可能不按预期执行工作 由于不正确的预设条件，可能会出现安全问题 通常，先决条件包括在受影响的代码部分的注释中 前提条件有时会在代码本身中使用 防护措施(guards) 或用 断言(assertions) 进行测试,有部分语言有特定的语法结构来测试 在契约设计中，一个软件元素可以假设前提条件得到满足，从而可以去除多余的错误检查代码 样例 Examples 契约式设计(design by contract)：没有针对前提条件的额外错误检查 防御性编程(defensive programming)：对预设条件进行额外的错误检查 继承中的前置条件 Pre-conditions in Inheritance 继承的方法的实现或重定义(方法覆盖&#x2F;method overriding)必须遵守该方法的继承契约(inherited contract) 前提条件可以在子类中被弱化(放宽),但是必须遵守继承契约 一个实现或重新定义可以减少一个方法的义务，但是不能增加它 后置条件 Post-Conditions 后置条件是一个条件或一个谓语，在执行完某段代码后必须保持为真 任何代码段的后置条件都是在代码段执行完成后保证的属性声明 通常，后置条件也会放在受影响的代码部分的文档中 后置条件有时会在代码本身中使用 防护措施(guards) 或 断言(assertions) 进行测试，在某些语言中有特定的语法结构用于测试 在契约设计中，只要软件的元素在前置条件为真的情况下被调用，则后置条件所声明的属性就会得到保证 继承中的后置条件 Post-Conditions in Inheritance 继承的方法的实现或重定义(方法覆盖&#x2F;method overriding)必须遵守该方法的继承契约(inherited contract) 后置条件可以在子类中 得到加强(更多限制), 而且必须遵守继承契约 一个实现或重新定义可以增加一个方法的利益(benefits)，但是不能减少它 例如： 原始契约要求返回一个集合 set重新定义的继承而来的方法返回排序后的集合，为方法提供更多的好处 类的不变量 Class Invariant 类的不变量约束了其存储在对象(object)中的状态(即某些变量的值) 类的不变量是在构造过程中建立的，并在调用公共方法(public method)之间不断维护。 类的方法必须确保类的不变性得到满足&#x2F;保留 在一个方法内，只要在公共方法结束前恢复了不变性，则方法内部的代码就可以破坏其不变性 类的不变量帮助程序员依赖有效的状态，避免了数据不准确(inaccurate)&#x2F;无效(invalid)的风险，也有助于在测试中定位错误 继承中的类的不变量 Class invariants in Inheritance 类的不变性是继承的，即： 一个类的所有父类的不变量&#x2F;不变性都适用于该类本身 一个子类可以访问父类的实现数据，但是必须始终满足所有父类不变量的不变性-&gt; 防止进入无效的状态 继承与契约式设计的总结 PreCondition 可以放宽，不能变得更严 (loosed) Post Condition 可以变的更严(更多限制) Invariant 会保持继承，即父类的不变量也适用于子类 设计原则 Design principles设计异味 Design smell 是不良设计的症状 往往是由于违反了关键的设计原则 应该在软件层面上重构(refractoring)来消除异味 设计异味的类型 Rigidity 即使是简单的更改，软件也有太难更改的倾向一次改动会导致其他依赖模块的连环改动 Fragility 当对一处位置进行更改时，软件有在多处发生故障的倾向 Rigidity 和 Fragility相辅相成：在需要新功能或变化时，力求将影响降到最低 Immobility 设计难以重复使用设计中有一部分可以用于其它系统，但是拆分系统所需的工作量和风险太大 Viscosity 软件粘性(software viscosity): 通过‘黑客’而非‘保留设计的方法’ 更容易实现变更环境粘性(environment viscosity)：开发环境缓慢且效率低下 Opacity 模块具有难以理解的倾向代码必须编写的清楚易懂 Needless complexity 包含目前无用的构造开发超前于需求 Needless repetition 设计包含重复的结构，这些结构可能能被统一简化到一个抽象概念中在重复单元中发现的错误必须在每次重复中修复 优秀设计的特点 Characteristics of good design好软件的目标是构建一个具有 loose coupling(松散耦合) and high cohesion(高度内聚) 的系统。从而让软件可以实现： 可扩展 Extensible 可重用 Reusable 可维护 Maintainable 可理解 Understandable 可测试 Testable 耦合 Coupling 定义为组件或类之间的 相互依赖程度 当一个 组件A 依赖于另一个 组件B 的内部运作，并受到 组件B 内部变化的影响时，就会出现 高度耦合(high coupling) 高度耦合会导致一个复杂的系统，其难以维护和拓展 应以 松散耦合(loosly coupled) 的类为目标-&gt; 允许组件之间独立使用和修改 零耦合(zero coupled) 的类是不可用的 聚合 Cohesion聚合代表 组件&#x2F;类&#x2F;模块中的所有元素 作为一个功能单元协同工作的程度 高度聚合的模块应该是：更易于维护，更改频率更低， 可重复使用性更高 不要把所有的责任都推给一个类来避免松散聚合(low cohesion) 设计原则 和 SOLID SOLID: 单一责任原则 (Single Responsibility principle): 一个类应该只承担一个责任 开-闭原则 (Open-closed principle): 软件应该对扩展开放而对更改封闭 里氏替换原则 (Liskov substitution principle): 程序中的对象应该可以使用其子类型的实例替换，且不会改变程序的正确性 如果一个类（或子类）被用作其超类的替代物，那么它应该能够完全替代超类，而不会引入不一致、不合理或不稳定的行为。 接口隔离原则 (Interface segregation principle): 多个专用接口要比一个通用接口更好 依赖倒置原则 (Dependency inversion principle): 应该依赖于 抽象(abstraction) 而不是 具体(concretion) 高层模块（高级模块或策略）不应该直接依赖于低层模块（底层模块或具体实现）。两者都应该依赖于抽象，也就是说，高层模块和低层模块都应该基于共同的接口或抽象类。通过使用接口或抽象类，可以减少模块之间的耦合，提高灵活性和可替代性。 何时使用设计原则 设计原则帮助消除设计异味 但是当不存在设计异味时不应使用设计原则 无条件遵守设计原则 过度遵从原则会导致设计异味-&gt; needless complexity 设计原则其一： 最小知识原则&#x2F;Demeter定律 principle of least knowledge&#x2F; Law of Demeter原则为：只和邻近的类产生交流 类应该和尽可能少的类发生交互 将类之间的交互减少到几个亲密的”朋友“ -&gt; 直接关联的类&#x2F;本地的对象(local objects) 便于设计 loosely coupled 系统 -&gt; 对系统某一部分的更改不会连带影响系统的其他部分 通过一系列规则限制交互 类中的方法只能调用以下的方法： 这个类本身的 作为参数传递到这个方法中的类的 方法中实例化的对象的 任何组件对象的 不使用 返回方法中的对象 禁止以下方法的调用：o.get(name).get(thing).remove(node) 规则1对象O 中的 方法M 可以调用 对象O 本身的任何其他方法 规则2对象O 中的 方法M 可以调用传递给 方法M 的参数中的任何方法 规则3当 方法M 实例化了 对象O，则 方法M 可以调用 对象O 中的 方法N 规则4对象O 中的任何 方法M 都可以调用作为 对象O 的直接组成部分的任何对象的任何方法-&gt; 一个类的方法可以调用其实例变量的类的方法 设计原则其二：里氏替换原则 LSP如果一个类（或子类）被用作其超类的替代物，那么它应该能够完全替代超类，而不会引入不一致、不合理或不稳定的行为。&#x2F; 子类必须可以替代其超类存在 不使用继承的设计 Solve the problem without inheritance除继承之外，还有其他的方法： 委托 Delegation -&gt; 将功能委托给另一个类 组合 Composition -&gt; 通过组合使用一个或多个类来重复行为 如果更多使用委托&#x2F;组合而不是继承的话， 软件会更灵活&#x2F;易于维护和拓展 方法重写规则 Rules of method overriding 参数应与被override的方法的参数完全相同 访问级别的限制不能超过被override的方法的访问级别 例如，如果超类的方法为 public,则子类中的覆盖方法就不能是private 或 protected 声明为 final 的方法不能被 override 构造函数不能被 override 静态方法和重写 static method and override 可以在子类中定义具有相同签名的静态方法 这本质上不是覆盖(override)，因为静态方法不存在运行时的多态性子类中的静态方法会隐藏超类的方法当在子类中定义一个与超类中的静态方法具有相同签名的静态方法时，它实际上是在子类中创建了一个全新的静态方法，而不会影响或替换超类中的静态方法。 返回值和重写 return value and override 重写的方法中的返回类型应该与超类中定义的返回类型相同 或 属于超类中返回类型的子类 重写的方法中的返回类型可以比父类的返回类型更窄 参数和重写 arguments and override当重写的方法中被传入的参数更宽泛&#x2F;不同时，java会认为这是两个不同的方法，而并没有override父类中的方法 重构 Refactoring重构的定义为：重组Restructuring(改变软件内部结构)软件，使其更易于理解easier to understand和使修改成本更低cheaper to modify，而不改变其外部可观察到的行为的过程 应当重构的情况：注意：当想要添加新功能时发现代码结构对添加新功能产生阻碍，应该先重构代码然后再添加新结构 添加函数时 需要修复bug时 进行代码回顾时 常见的代码异味 common bad code smells 重复代码 Duplicated code 同一代码结构出现在多个位置两个同级类中出现相同的表达式 过长的方法 Long Method 过大的类 Large class 当一个类是土佐太多事情时，往往会表现为具有太多实例变量(instance variables) 过长的参数列表 Long parameter list 分歧变化 Divergent change 通常发生在 一个类因为不同原因而出现不同变化时 Shotgun Surgery 通常发生在 需要对许多不同的类进行许多细小的更改 重构方法 Refractoring techniques方法1： 提取方法 Extract Method 查找逻辑代码块，并使用提取方法Extract Method 其中以switch代码块最为明显 查找方法中的本地变量 分辨变化和不变的局部变量 不变的变量可以作为参数被传入方法 任何被修改的变量都需要更加小心，如果只有一个变量，可以简单地返回 方法2： 重命名变量 Rename variable变量名应当清楚易懂 方法3： 移动方法 Move method通常来说，方法应当存在于其使用数据的对象上 方法4： 用查询替换临时变量 Replace Temp with Query 一种删除不必要的局部变量和临时变量的方法 在过长的方法中，临时变量会变得尤其隐蔽，很难跟踪到他们被用来做了什么 有时需要付出性能的代价 方法5： 用多态性取代条件逻辑 Replacing conditional logic with Polymorphism 在某些情况下，可以用继承等特性取代if逻辑 Extract Method &amp; Move Method 使用的OO 准测 通过 封装(Encapsulation) 和 委托(Delegation) 使代码可以被重复使用 封装可以保护数据封装可以保护行为-&gt; 当将行为从类中分离出来时，可以更改行为而无需改变类委托： 一个对象将操作转发给另一个对象，由另一个对象代表第一个对象执行的行为 重构(II)软件维护 software maintenance 软件系统随着时间的推移不断发展，以满足新的要求和功能 软件维护包括： 修正bug提升性能改进设计添加新功能 大部分的软件维护都注重于后三点 维护代码比从头开始编写更难 大部分开发时间用于维护 良好的设计、编码和规划可以减少维护的痛苦和时间 避免代码气味，减少维护痛苦和时间 代码异味与可能的迹象 Code Smells： Possible Indicators Duplicate code Poor abstraction(change one place -&gt; should change others) large loop&#x2F; method&#x2F; class&#x2F; parameter list&#x2F; deeply nested loop class with low cohesion modules with high coupling Class has poor encapsulation 封装性差 A subclass doesn’t use majority of inherited functions A “data class” has little functionality Dead code Design is unnecessarily general 设计过于笼统 Design is too specific 低级重构 low-level refactoring 命名 对方法&#x2F;变量重命名命名（提取）”神奇 “常量 程序 将代码提取到方法中将常用功能(包括重复代码) 提取到类&#x2F;方法中更改方法签名 重新排序 将一个方法拆分成多个小方法从而提高内聚性和可读性将语义上属于同一语句的语句放在一起 高级重构 high-level refractoring 使用设计模式 design pattern 改变语言习语(安全，简洁) 未知 unknown 存在两种困难问题 易于理解但难于解决易于解决但是难于理解 存在两种未知 已知的未知 (known unknowns)：知道它的存在但是不知道它是什么未知的未知 (unknown unknowns): 甚至没有想到需要考虑的未知 keep updating "},{"title":"2511note","date":"2023-06-03T11:49:05.000Z","url":"/2023/06/03/2511note/","tags":[["COMP2511","/tags/COMP2511/"]],"categories":[["notes","/categories/notes/"]],"content":"UNSW 2511 23T2 note keep updating 面向对象程序中的继承性• 继承(inheritance)是软件重用的一种形式，在这种形式中，新的类是由现有的类吸收其属性和行为而创建的。通过吸收它们的属性和行为，从现有的类中创建新的类 • 程序员可以指定新类继承现有类（称为 “超类(superclass)“）的属性和行为，而不是定义完全（独立）的新类。超类）。这个新类被称为**子类(subclass)**。 • 程序员可以为子类添加更多的属性和行为，因此,通常子类比它们的超类(superclass)有更多的功能。 继承关系形成树状的层次结构: is-a 继承关系• 在 “is-a“关系中，一个子类的对象也可以被当作超类的一个对象。 • 举例：undergraduateStudent 同样可被视作 student • 应该使用继承来模拟 “is-a“关系 is-a 继承关系注意事项除非所有或大部分继承的属性和方法都有意义，否则不要使用继承。 举例：在数学上，圆是一种椭圆，但是圆类不应该从椭圆类继承而来 -&gt; 一个椭圆类应该有一个方法设置宽度，另一个用来设置高度 Has-a 关联关系(Association relationship)• 在 “has-a“关系中，一个类的对象有另一个类的对象来存储它的状态或做它的工作，即它 “有一个“对该其他对象的引用。 • 举例：矩形并不是一条线，但是我们可以使用线来画出矩形 • has-a 关系 与 is-a 关系有很大的区别 • “Has-a“关系是通过现有类的组合来创建新类的例子(与扩展类 extending classes相反) has-a 继承关系注意事项• 在最终确定层次结构之前，应该考虑类的所有可能的未来用途 • 明显的解决方案有可能对某些应用不起作用 设计一个类 Designing a class 仔细思考一个类应该提供的功能&#x2F;方法 始终努力保持数据的私密性&#x2F;Private-&gt;(local) 创建一个对象可能需要不同的操作，如初始化 总是初始化数据 如果某个对象不再使用，则释放所有相关的资源 将有太多任务的类分散成多个小的类 类往往是密切相关的。 把共同的属性和行为 “分解”出来，并把它们放在一个类中。然后在类之间使用合适的关系（例如 “is-a “或 “has-a”） 类和对象的介绍• 类是数据和对该数据进行操作的方法（程序）的集合 • 举例：一个圆可以用它的中心的x、y位置和它的半径来描述。 • 可以定义一些关于圆的一些有用的方法(程序)，如计算周长&#x2F;面积等 • 通过定义圆 这个 类，即可创建一个新的 数据类型（data type） Class Circle在以下代码中，省略了 getter 和 setter 对象是类的实例(instance)在JAVA中， 对象是通过实例化一个类实现 举例： 访问对象的数据举例： 使用对象的方法 methods访问对象的方法类似于访问对象数据的语法 子类和继承 Subclasses and inheritancefirst approach创建一个新的独立的类 GraphicalCircle 并 重写已经存在于类 Circle 中的代码。 这是最差的解决办法 second approach这个方法使用了 “has-a“关系 意味着：一个GraphicalCircle 有一个 (数学上的) Circle 它使用 类 Circle 中的方法(area 和 circumference)去定义一些新的方法 这种方法也被称作方法转发 method forwarding third Approach这个方法使用了 “is-a“关系 将 GraphicalCircle 定义为 Circle 的 扩展(extension)或子类(subclass) 子类(subclass) GraphicalCircle 继承 其超类(superclass) Circle 中所有的变量和方法 我们可以将一个GraphicCircle的实体赋值给一个 Circle 类的变量： important: 假设有一个Circle类的变量 c 则只能访问在Circle类内部的属性和方法 不能对 c 使用 draw 方法 超类、对象和类的层次结构 class hierarchy 每个类都存在一个超类 superclass 如果不定义超类， 则默认为 object 类. Object 类 唯一一个没有superclass的类 由 object 定义的方法可以被任何 java 对象(实例instance)调用 通常需要重写 (override) 以下方法： toString() equals() hasCode() 抽象类 abstract classes 可以声明 仅定义部分实现(define only part of an implementation) 的类 使用扩展类(extended classes)来提供部分或全部方法的具体实现 使用的优点 可以声明(declare)方法，从而知道某个对象的接口定义(the interface definition) 在抽象类的不同子类中，方法可以以不同的方式实现 规则 抽象类是一个被声明为抽象的类 declared abstact 如果类包含抽象方法，则这个类必须被声明为抽象的 abstract method in absract class 抽象类不能被实例化 abstract class cannot be instantiated 如果一个抽象类的子类重写(override)其超类的全部抽象方法，并为这些方法提供了实现，则这个抽象类的子类就可以被实例化 override &amp; implement all abstract methods -&gt; instantiatable 若一个抽象类的子类没有实现其继承的所有抽象方法，则这个子类仍应该是抽象的 doesn’t implement all -&gt; still abstract 举例 注意事项： shape为一个抽象类，所以不能被实例化 Circle 和 Rectangle 的实例可以被分配给 Shape 的变量，且不需要casting， 即可理解为：Shape 的子类可以在不用casting的情况下被分配给 Shape 数组中的元素 可以对 Shape类 调用area() 和 circumference() 不需要casting的原因： 当使用父类引用变量访问子类对象时，编译器会自动识别并调用相应的子类方法（如果存在）。在这种情况下，数组 shapes 中的每个元素都是 Shape 类型的引用变量，但它们实际上引用的是 Circle、Rectangle 和 GraphicalCircle 对象。在循环中，通过 shapes[i].area() 调用了 Shape 类中的 area() 方法。由于多态性的存在，编译器会根据实际的对象类型（Circle、Rectangle 或 GraphicalCircle）来确定应该调用哪个子类的 area() 方法。 这样，无需显式进行类型转换（casting），编译器会根据对象的实际类型自动调用相应的方法。 单一继承和多重继承的比较 single inheritance vs multiple inheritance Java中，一个新的类只可以扩展一个超类 -&gt;单一继承 某些面向对象的编程语言支持多重继承，即一个新的类可以扩展两个或多个超类 对于多重继承，可能会出现超类中的一个行为被以多种形式继承(实现成了不同方法) Java中使用 interface 来实现多重继承 Casting 类型转换 定义：类型转换是将一个数据类型转换成另一个数据类型的过程，java中可以分为 隐式转换(iplicit casting)和 显式转换(explicit casting) 隐式转换(iplicit casting)在编译过程中自动进行的类型转换。它是安全的，不会导致数据丢失或溢出。隐式转换通常发生在以下情况下： 将一个较小的数据类型赋值给一个较大的数据类型。将字面量常量赋值给变量。 显式转换(explicit casting) 在编译过程中需要手动指定的类型转换。它用于将一个较大的数据类型转换为较小的数据类型。由于较大的类型可能无法完全容纳较小的类型的值，因此 可能会导致数据丢失或溢出。因此，在进行显式转换时，需要进行数据的精确性和边界检查。显式转换使用括号将要转换的数据类型括起来，并在前面加上目标类型的名称。 接口 interface 接口interface 类似于抽象类 abstract classes，但有几个重要的区别 在一个接口中定义的所有方法都是有隐藏的抽象的属性(并不需要使用abstract修饰符，但为了清楚明了仍建议使用) 在接口中声明的变量必须是 static&#x2F;final 的，即均为常量 类似于一个类可以拓展其超类一样， 它也可以选择实现一个接口 为了实现一个接口，一个类必须首先在一个 implements 字句中声明这个接口， 然后必须实现接口所有的抽象方法 一个类可以实现多于1个接口 一个接口可以同时继承多个接口 (Interface can extend multiple interfaces) 举例： 定义： 使用：当一个类实现了一个接口， 则这个类的实例也可以被分配给变量类型为接口类型的变量： 实现多个接口 implement multiple interfaces一个类何以实现多于1个的接口 拓展接口 extend interfaces 接口可以有子接口(sub-interfaces), 就像类可以有子类一样 一个子接口继承了其 父接口(super-interface) 的所有抽象方法和常量，并且可以定义新的抽象方法和常量 接口可以同时拓展多个接口 can extend more than one interface at a time 方法转发 Method Forwarding 假设 类C 扩展了 类A, 并且实现了 接口X 由于 接口X 定义的所有方法都是抽象的，所以 类C 需要实现所有的方法 然而， X 有三种实现方式 (分别是 P&#x2F;Q&#x2F;R) 在 类C 中，我们可能想使用这些实现中的一个 -&gt; 想使用 P&#x2F;Q&#x2F;R中实现的部分或者全部方法 比如，如果想使用 P 中的方法， 可以通过在 类C 中创建一个类型为 P 的对象来实现， 并通过这个对象访问 P 中实现的所有方法 在 类C 中，我们需要为 接口X 中的所有方法提供必要的存根(stub)， 在编写方法时， 可以简单地通过 类P 的对象调用 类P 的方法 以上即为 方法转发 Method Forwarding 方法重写(多态性) Method Overriding(Polymorphism) 当一个类定义了一个使用相同名称,返回类型的方法，并且其参数的数量，类型和位置于其超类中的方法完全相同时，这个类中的方法会覆盖掉超类的方法，即为override。 如果这个方法被该类的一个对象调用，则被调用的是这个方法的新的定义，而不是超类中的旧定义 多态性 polymorphism 一个对象根据其在继承层次中的位置，决定对自己使用何种方法的能力，通常被成为多态性 方法重写举例：在下面的例子中：* 若p为 类B 的实例，则 p.f() 为 类B 中的 f() 若p为 类A 的实例，则 p.f() 为 类A 中的 f() 这个例子还将展示如何使用 super 关键字来引用被覆盖掉的方法 假设 类C 为 类B 的 子类， 且 类B 为 类A 的 子类类A 和 类C 都定义了 方法f()对于 类C，我们可以通过以下方法调用被覆盖掉的方法 f() : 但是： 如果三个类都定义了 f(), 则在 类C 中调用 super.f() 将会调用 类B 中定义的方法 尤其重要的是，在上述的情况下，没有办法在 类C 中使用 A.f() super.super.f() 在java语法中是不被允许的 方法重载 method Overloading 定义具有相同名称，不同参数或者返回类型的方法被称作 方法重载 在 JAVA 中， 通过区分方法的名称，返回类型，以及它参数的数量&#x2F;类型&#x2F;位置 来区分方法 数据隐藏和封装 Data Hiding and Encapsulation 可以将数据隐藏在类中，并且只能通过类的方法使其可用 可以帮助保持对象数据的一致性 -&gt; state of an object 可见性修改器 visibility modifiersJAVA提供物种访问修改器 access modifiers -&gt; 对 variable&#x2F;method&#x2F;class 构建器 constructors 良好的做法是为所有的类定义所需要的构造函数如果一个类没有定义构造函数，则： 无参数（no argument）构造函数被隐式插入这个无参数构造函数会调用超类的无参数构造函数如果超类没有一个可见的（visible）无参数构造函数，则会导致编译错误 如果构造函数的第一条语句不是对 super() 或者 this() 的调用，则会隐含地插入对 super() 的调用如果一个构造函数被定义为有一个或者多个参数，则无参数构造函数不会被插入到这个类里面一个类可以有多种具有不同签名（signatures）的多个构造函数this 这个词可以用来调用同一个类中的另一个构造函数 举例 钻石型的继承问题 Diamond inheritance Problem在JAVA中使用单一继承(single inheritance) 通过以下方式实现： 在 类Z 中，使用定义于 类X 和 类W 中的方法和变量 在 类Z 中，如果想使用 类Y 中实现的方法， 可以通过使用方法转发 method forwarding，即，在 类Z 中创建一个 类型为 类Y 的对象，然后在 类Z 中可以通过这个对象访问在 类Y 中定义的 方法 Z 和 Y类 的对象可以分配给 IY 类型的变量，而不是 Y类型 的变量 (即使用 IY 作为变量类型，使用casting实现) Java中的Exceptions Exception 是一个事件，它发生在一个程序的执行过程中，扰乱了程序指令的正常流程 当错误发生时，一个异常对象被创建并交给实时运行的系统，此为 抛出一个异常(throw an exception) 运行时系统在调用stack中搜索一个方法，该方法包含一个可以处理该异常的代码块 所选择的异常处理程序被称为捕获异常(catch the exception) checked&#x2F;unchecked exceptions有三种Exceptions： Checked exception (IOException&#x2F; SQLException…) Error (VirtualMachineError, OutOfMemoryError…) Runtime exception(ArrayIndexOutOfBoundsExceptions&#x2F; ArithmeticException…) checked vs unchecked Exceptions Exception 的类型决定了他是 checked&#x2F;unchecked 所有属于 RuntimeException (通常由程序代码中的缺陷引起)&#x2F; Error (通常是”系统”问题) 子类的都是 unchecked exception 所有继承自 Exception类 但不直接或间接继承自 RuntimeException类 的类都被认为是checked Exception JAVA中exception的层次结构 Hierarchy of Java exceptions 样例 在上述代码中，首先会尝试运行try部分， 如果有任何异常则终止运行并抛出exception，进入catch部分， 如果抛出的exception类型和catch中的类型相对应，则进入对应的catch模块， 最后，无论是否引发exception，都会执行finally部分 用户定义的Exceptions exceptions可以被自定义 所有的exceptions都必须是 Throwable的子类 一个 checked exception需要扩展 Exception类，但不能直接或间接地从RuntimeException类中获取 一个unchecked exception(例如runtime exception)需要从runtimeException类中扩展出来 通常情况下。我们通过扩展 Exception类 来定义一个checked Exception 继承中的Exceptions Exceptions in Inheritance 如果一个子类方法覆盖(override)一个超类方法，子类的throws子句可以包含超类 throws字句的一个子集但是不能抛出更多的exceptions JAVA中的Assertions Assertion是java的一个语句，可以使用其测试对程序的假设是否成立 Assertion在检查下列情况时很有用： 前置条件(preconditions)后置条件(post-condition)类不变量(invariant -&gt; 根据design by contract)内部不变量 (internal invariants)控制流不变量(control-flow invariants) 不应用于检查下列情况： 用于public methods的参数检查做任何程序需要正确操作的工作 评估assertion不应导致副作用(side effects)的产生 样例： Exception总结 在设计过程(design process)中应该考虑异常处理和错误恢复策略 有些情况下可以通过先验证数据来防止异常的发生 如果一个Exception可以在一个方法中得到有意义的处理，那么这个方法应该可以捕获(catch)这个exception，而不是单纯的声明它(should catch rather than declare) 如果一个子类方法覆盖(override)一个超类方法，那么子类throw的字句可以包含超类throw字句的一个子集，但是绝对不能抛出更多的异常 程序员应该处理 checked exceptions 如果预期中会出现 unchecked exception， 必须仔细处理这些exceptions 只有第一个匹配的catch才会被执行，所以要仔细确定catch的类 exception是API文档和契约的一部分 assertion 可以用来检查前置条件&#x2F;后置条件和不变量 java中的泛型和集合 Generics and Collections in JAVAjava中的泛型 Generics in java泛型(Generics) 使类型types(类和接口) 在定义以下时可以成为参数： 类 classes 接口 interfaces 方法 methods 优点： 移除casting并在编译时提供更强的类型检查(type check) 允许通用算法(Generic algorithms)的实现，这些算法适用于不同类型的集合，可以被定制，并且类型是安全的 通过使更多的错误在编译时被发现，增加了代码的稳定性 Without Generics With Generics Generic Types 通用类型&#x2F;泛型 泛型是一个泛型类或接口，他在类型上有参数化(parameterized) 通用类(generic class)的定义格式: 最常用的类型参数名称为： E - element(被java collection framework广泛使用)K - keyN - NumberT - TypeV - valueS,U,V etc. - 2nd,3rd,4th types normal and Generic version normal version Generic version 创建变量 多类型参数 Multiple Type Parameters 一个泛型类(generic class) 可以有多个类型参数 例如，通用的OrderedPair类，有通用的配对接口 使用样例： 通用方法 Generic Methods 通用方法是指 引入自己的类型参数的方法 -&gt; introduce own type 调用上述方法的完整语法为： java中的集合 collections in java 集合框架(colelctions framework)是一个统一的架构， 用于表示和操作集合。 集合只是一个将多个元素组合成一个单元的对象 collection is an object 所有集合框架都包含以下内容： 接口：允许独立于其表示的细节来操作集合实现：集合接口的具体实现算法：对实现集合接口的对象进行有用的计算方法，如搜索&#x2F;排序 这些算法被认为是多态的(polymorphic): 同一个方法可以用在适当的集合接口的许多不同的实现上 核心集合接口 Core Collection interfaces 核心集合接口封装了不同类型的集合 这些接口允许对集合的操作独立于他们的表示细节 集合接口 the collection interface 一个集合(collection)表示一组被称为其元素的对象 集合接口(collection interface)被用来在需要最大通用性的地方传递对象的集合 例如，按照惯例，所用通用的集合实现都有一个构造函数，其需要一个Collection 参数 集合接口包含执行基本操作的方法，例如： 集合的实现 collection implementation下表概述了通用的实现方式： JUnit Testing软件测试 software testing 不同类型的测试： 面向对象的设计文件描述了类和方法的职责(API) -&gt; unit testing 单元测试系统设计文件(System design document) -&gt; integration testing 集成测试需求分析文件(Requirements Analysis Document) -&gt; System Testing 系统测试客户期望(Client Expectation) -&gt; Acceptance Testing 验收测试 单元测试对于重构(refactoring)任务很有用 JUnit JUnit是一个流行的单元测试框架，用于测试java程序 大多数流行的IDE都能方便地集成JUnit 基本的JUnit术语： 测试案例 Test cases - 包含测试方法的 Java class测试方法 Test methods - 在测试案例中执行测试代码的方法，用 @Test 注释Asserts - Assert&#x2F;Assert statement检查预期结果与实际结果的对比测试套件 Test Suites - 多个测试案例的集合 JUnit Example assertEquals assertTrue Exception Java Lambda 表达式 Lambda表达式允许我们： 轻松定义匿名方法 Anonymous methods视代码为数据 code as data将功能作为方法参数 functionality as argument 只有一个方法的匿名内部类可以用lambda表达式代替 Lambda表达式可被用来实现 只有一个抽象方法的接口， 这种接口被称为 功能接口 Functional interfaces Lambda表达式将函数作为对象提供 functions as objects Lambda表达式更简洁，灵活度更高 lambda 表达式的语法 Syntax一个lambda表达式包含以下内容： 用括号括起来，以逗号分隔的正式参数列表。 无需提供数据类型。 若只有一个参数则可以省略括号 箭头符号 -&gt; 主体，由单个表达式或语句块构成 方法引用 Method References我们可以将现有方法视为功能接口的实例 使用::操作符可以引用方法： 静态方法Static method: (ClassName :: methodName) 特定对象的实例方法： (instanceRef :: methodName) 类的构造函数： (ClassName :: new) Java中的功能接口 Function interface java.util.function 包中的功能接口为 lambda 表达式和方法引用提供了预定义的目标类型 每个功能接口都有一个抽象方法，称为该功能接口的功能方法，lambda表达式的参数返回类型与该抽象方法相匹配或适应 功能接口可以在多种上下文中提供目标类型，如赋值，方法调用等 有几种基础的功能形状 function shapes: Function (从T映射到R) 包含一个名为 apply 的抽象方法，该方法接受一个类型为 T 的参数，返回一个类型为 R 的结果。 Consumer (接受输入类型 T，没有返回值) 它包含一个名为 accept 的抽象方法，该方法接受一个类型为 T 的参数，执行相应的操作 Predicate (接受输入类型T,返回布尔值) 它包含一个名为 test 的抽象方法，该方法接受一个类型为 T 的参数，返回一个布尔值表示断言的结果。 Supplier (没有输入值，返回输出类型 R) 它包含一个名为 get 的抽象方法，该方法不接受参数，返回一个类型为 R 的结果。 使用Lambda功能的比较器 Comparator Pipeline &amp; Streams Pipeline 是一系列集合操作(aggregate operation) 以下实例打印了集合名册中包含的男性成员，该pipeline包括聚合操作 filter 和 forEach 在pipeline中，各个操作是loosely coupled,其只依赖于输入的数据流，这些操作可以很容易地重新排列&#x2F;替换成其他合适的操作 上述语法中的 . 符号与用于实例或类的. 符号的意义完全不同 一个pipeline包含以下组件： 一个源 Source：可以是一个集合collection，数组array, 发生器函数generator function, IO通道I/O channel.零个或多个中间操作，中间操作如fliter会产生一个新的数据流stream 数据流 Stream 是一个元素序列(sequence of elements), stream方法 可以从一个集合中创建一个流 过滤操作filter会返回一个新的数据流，其中包含与其谓词相匹配的元素。 终端操作(如 forEach)会产生一个非流结果，如一个原始值（如 double 值）、一个集合collection，或者在某些情况下，根本不会产生任何值。 设计模式 Design Pattern 设计模式是针对经常出现的问题的一种尝试性解决方案 每个模式都有： 简短的名称背景描述问题描述解决方案 在软件工程中，设计模式是针对软件设计中经常出现的问题的一般可重复的解决方案 设计模式是： 代表如何解决问题的模板捕捉设计方面的专业知识，并使这些知识得以传递和重复使用提供共享词汇表，改善沟通，简化实施不是最终的解决方案，而是设计问题的一般解决方案 设计模式的类别： 行为模式 Behavioural Patterns结构模式 Structural Patterns创造模式 Creational Patterns 策略模式 Strategy Pattern 允许将一系列算法封装encapsulate起来 允许在运行过程中改变行为(更改策略) This pattern defines a family of algorithms, encapsulates each one -&gt; 策略模式定义了一系列算法并将它们封装起来 策略模式是一种行为设计模式Behavioural pattern，可使算法独立于使用它的类而变化 实现： 策略模式的适用性，优点和缺点 Applicability&#x2F; benefits&#x2F; drawbacks 适用性(在以下情况下时适用) Applicability 许多相关类别的行为各不相同一个 类 可以从算法的不同变体中受益一个类定义了许多行为，这些行为以多个条件语句（如 if 或 switch）的形式出现。运用策略模式可以将每个条件分支移到各自具体的策略类内部中 优点 Benefits 使用组合(composition)而非继承(inheritance)，从而使行为和使用行为的上下文类之间更好地解耦 (Decoupling) 缺点 Drawbacks 增加对象数量客户必须了解不同的策略 举例 Examples 对列表排序(quicksort &#x2F; bubble sort &#x2F; merge sort) 将每种排序算法封装为一个 具体策略类类在运行时决定 需要哪种排序行为 搜索 (binary search &#x2F; BFS &#x2F; DFS) 状态模式 State Pattern有限状态机 Finite-state Machine 有限状态机（FSM）是一种抽象机器，它在任何给定时间内都能处于有限状态中的一种状态。 有限状态机可以根据某些外部输入从一种状态转换到另一种状态从一种状态到另一种状态的变化称为过渡 transition 有限状态机的定义是 一个状态列表每个过渡的条件其初始状态 举例：投币式旋转门 状态机中的术语 Terminology 状态 State: 对等待执行转换的系统状态的描述 过渡 Transition: 在满足条件或收到事件时要执行的一组操作 根据当前的状态不同，相同的行为可能会触发不同的行为 通常来说，以下内容也与状态相关： 进入动作 Entry action: 进入状态时执行退出动作 Exit action: 退出状态时执行 表现形式 Representation 实例分析 Gumball machinebad design 对于 gumball machine，有四种状态： No quarter Has quarter gumball sold out of gumball 创建一个变量保存当前状态，并为每个状态定义值： 整合系统中所有可能发生的操作： insert quarter turns crack eject quarter dispense 现在创建一个类来代表状态机，对于每种操作，创建一个方法并适用条件判断来根据不同的状态执行行为，例如以下： 对于以上的设计，每当有一个新的状态添加进来，都需要更改全部的操作的代码，所以我们需要重新设计整个系统 better design 对于之前的设计，我们将重新设计它，将不同的状态对象封装在其自身的类中，并在发生操作时委托给当前的状态步骤： 定义一个状态的接口，其中应包含gumball machine中每个操作的方法 对于每个状态，定义并实现一个属于其自己的类，当机器处于对应的状态时，这些类将负责机器的行为 去掉动作中的所有的条件代码，转而委托给状态类来实现不同的行为 对于不同的状态，都会有针对不同行为的方法，举例如下： 要点 状态模式允许一个对象根据其内部状态做出许多不同的行为 与过程式状态机不同，状态模式将状态表示为一个完整的类 类通过委托给与其组成的当前状态对象来获取行为 通过将每个状态封装到一个类中，我们可以本地化任何需要进行的更改 状态模式和策略模式的类图相同，但意图不同 策略模式通常用一种行为或算法配置上下文类 状态模式允许上下文在状态发生变化时改变其行为 状态转换可由状态类或上下文类控制 使用状态模式通常会增加设计中的类的数量 状态类可在上下文实例之间共享 与策略模式的区别在状态模式中，特定状态知道其他所有状态的存在，且能触发从一个状态到另一个状态的转换策略则几乎完全不知道其他策略的存在。 观察者模式 Observer Pattern 观察者模式用于在 “事件驱动 “编程中实现分布式事件处理系统 在观察者模式中： 一个被称为主体subject（或可观察对象 observable 或发布者 publisher）的对象，会维护其被称为观察者observers（或订阅者 subscribers）的隶属者列表主体会 自动 将其的任何状态变化 通知 观察者，通常是通过调用观察者的方法之一 观察者模式定义了对象之间一对多的依赖关系，因此当一个对象（主体）改变状态时，其所有依赖对象（观察者）都会 自动得到通知和更新 观察者模式的目的应为： 在对象之间定义一对多的依赖关系，而不使对象紧密耦合当主体改变状态时，自动 通知&#x2F;更新 数量不限 的观察者 (从属对象)能够 动态 添加和删除观察者 可能的解决方案 Possible solution 定义 “主体Subject“和 “观察者Observer“接口，当主体改变状态时，所有注册的观察者都会收到通知并自动更新 主体 的职责是维护一个观察者列表，并通过调用观察者的 update() 操作将状态变化通知观察者 观察者 的职责是在主体上注册（和取消注册）自己（以获得状态变化的通知），并在收到通知时更新自己的状态。 这样的方式令 主体和观察者之间保持着 loosely coupled 的关系 可在运行时独立 添加 和 移除 观察者 多观察者和主体 传递&#x2F;更新数据主体需要传递（更改）数据，同时向观察者发出更改通知，有两种可能的选择 Push data 主体将更改后的数据传递给其观察者 update(data1, data2...)所有观察者都必须实现上述更新方法 Pull data 主体将自身的参照传递给观察者，观察者需要从主体获取（提取）所需的数据 update(this)主体需要为其观察者提供所需的访问方法 public double getTemperature() 观察者模式的结构 总结优势 避免主体与观察者之间的紧密耦合 (avoid high coupling) 令主体和观察者就可以在系统中处于不同的抽象层次。 松散耦合对象更易于维护和重复使用 允许动态注册和注销注册 需要注意的 主体的变化可能导致对其观察者的一连串更新，并进而导致对其从属对象的更新，从而产生复杂的更新行为 需要妥善管理此类依赖关系 要点 观察者模式定义了对象之间的单对多关系 主体或我们所熟知的观测对象，使用一个通用接口更新观察者的数据 观察者是松耦合的，因为可观察对象对它们一无所知，只知道它们实现了观察者接口 在使用模式时，您可以从可观察对象中推送或提取数据(pull被认为是更正确的) pull允许观察者在需要的时候获取数据，避免了频繁的推送和可能的资源浪费。5.不要依赖于Observer的特定通知顺序 在需要的情况下，可以自行创建Observable类并实现 复合模式 Composite Pattern 在 OO 程序设计中，复合对象是由一个或多个类似对象（具有类似功能）组成的对象 目的是让我们能够像操作一组对象一样操作单个对象实例，例如 调整一组形状大小的操作应与调整单个形状大小的操作相同计算文件的大小应与计算目录的大小相同 单一（叶）对象与复合（组）对象之间没有区别 如果我们区分单个对象和一组对象，代码就会变得更加复杂，因此也更容易出错 样例计算单个零件或完整子组件（由多个零件组成）的总价，而不必区分零件和子组件 可能的解决方案 Possible solution 为叶子Leaf（单个&#x2F;部分）对象和组合Composite（组&#x2F;整体）对象定义统一的组件界面 组合对象中存储了一系列子组合(Leaf&#x2F; Composite) 客户端可以忽略对象组合与单个对象之间的差异，这大大简化了复杂层次结构的客户端，使其更易于实施、更改、测试和重用 树形结构通常用于表示部分-整体层次结构。 多向树形结构在每个节点（下面的子节点）上都存储了一个组件集合，用于存储叶子对象和复合（子树）对象 叶Leaf型的对象直接对对象本身进行操作 复合对象(COmposite)对其子对象执行操作，并在需要时收集返回值并得出所需的答案 实现问题： 统一性Uniformity和 类型安全Type Safety 类型安全设计：只在复合类中定义与子类相关的操作 统一性设计：在组件界面中包含所有与子代相关的操作 统一性 在组件接口中包含所有与子代相关的操作，这意味着叶子类需要以 “什么也不做 “或 “抛出异常 “的方式来实现这些方法 客户端可以统一处理叶子对象和复合对象 我们会失去类型安全性，因为叶子类型和复合类型并没有干净地分开 适用于子节点类型动态变化（从叶子节点到复合节点，反之亦然）的动态结构，客户端需要定期执行与子节点相关的操作。例如，文档编辑器应用程序 更适合节点类型经常反复变化的情况 类型安全 只在复合类中定义与子类相关的操作 类型系统会强制执行类型限制，因此客户端无法对 Leaf 对象执行与子对象相关的操作 客户端需要区别对待叶子对象和复合对象 适用于静态结构，在这种结构中，客户端不需要对 “未知” 的组件类型对象执行与子对象相关的操作 总结 复合模式提供了一种结构，既可容纳单个对象，也可容纳复合对象 复合模式允许客户统一处理复合材料和单个对象 组件Component是复合结构中的任何对象，组件可以是其他 复合节点Composite node或叶节点Leaf node 在实施 Composite 的过程中，有许多设计上的权衡。 您需要在透明度&#x2F;统一性和类型安全性与您的需求之间取得平衡 创造模式创建模式提供了各种对象创建机制，提高了现有代码的灵活性和重用性 工厂模式 Factory Method 提供了在超类中创建对象的接口、 但允许子类改变创建对象的类型 抽象工厂模式 Abstract Factory method 让用户在不指定具体类别的情况下生成相关对象 构建者 Builder 让用户逐步构建复杂的对象。该模式允许用户使用相同的构造代码生成不同类型和表现形式的对象 单例模式 Singleton 让用户确保一个类只有一个实例, 同时为该实例提供一个全局访问点 工厂模式 Factory Method 工厂方法是一种创建设计模式，它使用工厂方法来处理创建对象的问题，而无需指定将要创建的对象的确切类别 问题 直接在需要（使用）对象的类中创建对象缺乏灵活性它将类与特定对象绑定使实例化无法独立于（无需改变）类而改变 可能的解决方法 为创建对象定义一个单独的操作（工厂方法）通过调用工厂方法创建对象这样就可以编写子类，改变创建对象的方式（重新定义实例化哪个类） 结构 structure Product 声明了创建者(Creator)及其子类可生成的所有对象所共有的接口 具体产品是产品接口(Product interface)的不同实现方式 创建者(Creator)类声明了返回新产品对象的工厂方法createProduct() 具体创建者会覆盖基本工厂方法createProduct()，从而返回不同类型的产品 样例 Example 如图，Dialog 类作为factory会在工厂方法 createButton() 中返回不同类型的Button，具体的返回类型取决于子类的类型 抽象工厂模式 Abstract factory pattern 抽象工厂是一种创建设计模式，它能让你创建相关对象的家族，而无需指定它们的具体类 问题 problem想象一下，您正在创建一个家具店模拟器，您的代码包含以下内容 &gt; 相关产品系列，例如 椅子 + 沙发 + 咖啡桌 &gt; 该系列的几个变种 &gt; 例如，椅子 + 沙发 + 咖啡桌产品有以下几种款式 &gt;&gt; 艺术类 &gt;&gt; 维多利亚式 &gt;&gt; 现代式 解决方法 solution 首先， 抽象工厂模式建议为系列中的每件产品明确声明接口 （例如椅子、 沙发或咖啡桌）。 然后， 确保所有产品变体都继承这些接口。 例如， 所有风格的椅子都实现 椅子接口； 所有风格的咖啡桌都实现 咖啡桌接口， 以此类推 接下来， 我们需要声明抽象工厂(abstract factory)——包含系列中所有产品构造方法的接口。 例如 create­Chair()创建椅子 、 ​ create­Sofa() 创建沙发和 create­Coffee­Table() 创建咖啡桌 。 这些方法必须 返回抽象产品类型 ， 即我们之前抽取的那些接口： ​ 椅子,沙发和咖啡桌等等 对于系列产品的每个变体,我们都将基于 抽象工厂接口(abstract factory interface)创建不同的工厂类。每个工厂类都只能返回特定类别的产品， 例如,现代家具工厂Modern­Furniture­Factory只能创建 现代椅子Modern­Chair、​ 现代沙发Modern­Sofa和 现代咖啡桌Modern­Coffee­Table对象 客户端代码可以通过相应的抽象接口调用工厂和产品类。 你无需修改实际客户端代码， 就能更改传递给客户端的工厂类， 也能更改客户端代码接收的产品变体。 客户端无需了解工厂类 结构 Structure 抽象产品(abstract product)为组成产品系列的 一系列不同但相关的 产品声明接口 具体产品(concrete product)是抽象产品的各种实现，按变体分组。每个抽象产品（椅子&#x2F;沙发）必须在所有给定的变体（维多利亚式&#x2F;现代式）中实现 抽象工厂(abstract factory)接口 声明 了一套方法，用于创建每个抽象产品 具体工厂(concrete factory) 实现 抽象工厂的创建方法。每个具体工厂对应一个特定的产品变体，并 只创建这些产品变体 客户端可以与任何具体的工厂&#x2F;产品变体合作，只要它通过抽象接口与它们的对象通信即可 与工厂模式的区别 difference with factory pattern 目的和用途 工厂模式旨在通过将对象的创建封装到一个共同的接口中，以便根据需求创建不同类型的对象，同时隐藏对象的实例化细节抽象工厂模式旨在提供一个接口，用于 创建相关或依赖对象的系列 ，而无需指定具体的类。它允许客户端创建一组相关对象，而不必关心具体的类名 结构和关系 工厂模式通常包含一个工厂接口（或抽象类），以及具体的工厂类，每个具体工厂类负责创建特定类型的对象抽象工厂模式包含一个抽象工厂接口（或抽象类），定义了一系列可以 创建不同类型对象的方法，每个具体工厂类实现了这个接口，用于创建特定系列的对象 对象创建的区别 工厂模式关注于对单个对象的创建抽象工厂模式关注于对一组相关对象的创建，这些对象之间可能存在关联 扩展性 工厂模式相对较容易添加新的具体工厂类，以支持创建新的产品类型抽象工厂模式相对较容易添加新的具体工厂类，以支持创建新的产品系列，但不太容易添加新的产品类型 装饰器模式 Decorator pattern 装饰器设计模式允许我们在运行时根据需求有选择地为对象（而不是类）添加功能 初始的类的行为不变（开放-封闭原则） 继承会在编译时扩展行为，附加功能会在该类的所有实例的生命周期中绑定 与继承相比，装饰器设计模式更倾向于组合(composition)。它是一种结构模式(structural pattern)，为现有类提供了一个包装器(wrapper) 由于这种设计模式涉及递归，因此可以按不同顺序多次装饰对象*无需在单一（复杂）类中实现所有可能的功能 结构 structure 部件 Component 声明封装其和被封装对象的公用接口 具体部件 Concrete Component 作为被封装对象所属的类，它同时定义了部件的基础行为execute，但是装饰类可以更改此类行为 基础装饰 Base Decorator 拥有一个指向被封装对象的引用成员变量(wrappee), 该变量的类型为通用部件接口(Component)，通过这样的声明，可以引用具体的部件和装饰。 装饰基类会将所有操作委派给被封装的对象 具体装饰类 Concrete Decorator 定义了可动态添加到部件的额外行为。 具体装饰类会**重写装饰基类的方法 extcute**， 并在调用父类方法之前或之后进行额外的行为 Java中的泛型 Generic Types有界类型参数 Bounded Type Parameters 有时，您可能需要限制参数化类型中可用作类型参数的类型 例如，对数字进行操作的方法可能只接受 Number 或其子类的实例 多重限制 multiple bounds 一个类型参数可以有多个边界 &lt; T extends B1 &amp; B2 &amp; B3 &gt; 具有多个边界的类型变量是边界中列出的所有类型的子类型 注意，上面的 B1、B2、B3 等指的是接口或类。最多只能有一个类（单一继承），其余（或全部）都是接口 如果其中一个边界是一个类，则必须首先指定该类 错误样例 正确样例 泛型，继承和子类 Generics, inheritance and subtypes 给定以下方法 public void boxTest( Box&lt;Number&gt; n ) &#123; /* ... */ &#125; 其支持传入的变量既不能是Box&lt;Integer&gt;，也不能是 Box&lt;Double&gt; 因为 Box&lt;Integer&gt; 和 Box&lt;Double&gt; 都不是 Box&lt;Number&gt;的子类 想让方法能接受 Box&lt;Integer&gt; 和 Box&lt;Double&gt;，可以将方法改为： public void boxTest( Box&lt;? extends Number&gt; n ) &#123; /* ... */ &#125; 通过使用通配符类型 ? extends Number，您可以接受任何类型为 Number 或其子类的 Box 对象作为参数 通用类和子类 Generic classes and subtyping 可以通过扩展或实现泛型类或接口来对其进行子类型化 一个类或接口的类型参数与另一个类或接口的类型参数之间的关系由 extends 和 implements 子句决定 举例： ArrayList&lt;E&gt; implements List&lt;E&gt;List&lt;E&gt; extends Collection&lt;E&gt; 因此，ArrayList&lt;String&gt; 是 List&lt;String&gt; 的子类型，而 List&lt;String&gt; 是 Collection&lt;String&gt; 的子类型。 只要不改变类型参数、 类型之间的子类型关系就会得到保留 通用配符 上界 wildcards: upper bounded 在通用代码中，问号 (?) 被称为通配符，代表未知类型 通配符可以在多种情况下使用：作为 参数、字段或局部变量的类型 ；有时作为返回类型 上界通配符 &lt;? extends Foo&gt;（其中Foo是任意类型）匹配Foo和Foo的任意子类型。 可以指定通配符的上限(upper bounded)，也可以指定下限(lower bounded)，但不能同时指定 通用配符 无界 wildcards: unbounded 无限制通配符类型使用通配符 (?) 指定 例如，List&lt; ? &gt;. 这称为未知类型的列表 通用配符 下界 wildcards: lower bounded 上界通配符将未知类型限制为特定类型或该类型的子类型，并使用 extends 关键字表示 下限通配符使用通配符（?）表示，后面是 super 关键字，后面是其下限： &lt; ? super A &gt; 要编写适用于 Integer 列表和 Integer 的超级类型（如 Integer、Number 和 Object）的方法，您需要指定 List&lt;? super Integer&gt; 方法 List&lt;Integer&gt; 比 List&lt;? super Integer&gt; 更具限制性 通用配符和子类型 wildcards and subtyping 虽然 Integer 是 Number 的子类型，但 List&lt;Integer&gt; 并不是 List&lt;Number&gt; 的子类型，这两种类型并不相关 List&lt;Number&gt; 和 List&lt;Integer&gt; 的共同父类是List&lt;?&gt;。 测试设计 Test design软件测试：测试推动开发 Test-Driven Development (TDD) 软件开发过程中的每次迭代之前都必须制定计划，以适当验证（测试）所开发的软件是否满足要求（即后置条件） 软件开发人员不能在开发出软件后才考虑如何对其进行测试 测试是开发软件解决方案不可或缺的重要组成部分。绝不能事后才考虑测试 在每次迭代过程中，必须根据测试套件对增量开发进行测试。每次代码修改和&#x2F;或重构后，都必须使用预定义的测试套件进行适当测试。 在开始实施解决方案之前，必须根据需求规格设置测试。 软件的开发顺序应为 要求&#x2F;规格分析 接口 (方法签名&#x2F;前置&amp;后置条件) 测试集合 (单元测试&#x2F;集成测试) 方法实现 软件测试：输入空间覆盖 Input Space Coverage 不能通过试错的方式随意进行测试。 测试必须系统地进行，并制定周密的测试计划。 目标应是考虑可能的输入空间，并尽可能覆盖它。 通常的做法是将输入空间划分为 “等价组 equivalence groups“，并从每个等价组中选择一个有代表性的输入。这里的假设是：在同一等价组中，程序在每个输入上的行为都是相似的。 考虑边缘情况borderline，通常称为边界测试boundary testing 对于多个输入值，考虑可能的输入组合，对其进行优先排序，并在时间和资源允许的情况下考虑尽可能多的输入组合。同样，将可能的组合划分为同质子集，并选择具有代表性的组合 软件测试：代码覆盖率 Code Coverage 代码覆盖率是一个有用的指标，可以帮助你评估测试套件的质量。 代码覆盖率通过确定测试套件成功验证的代码行数，衡量软件通过测试套件验证的程度。 大多数覆盖率报告中的常见指标包括 函数覆盖率 function coverage：有多少已定义的函数被调用。 语句覆盖率 statement coverage：程序中有多少语句被执行。 分支覆盖率 branches coverage：执行了多少个控制结构分支（例如 if 语句）。 条件覆盖率 condition coverage：测试了多少布尔子表达式的真假值。 行覆盖率 line coverage：测试了多少行源代码。 软件测试和模拟中的随机性 Randomness in Software Testing and Simulation 软件测试： 随机数据通常被视为无偏数据 提供平均性能（如在排序算法中） 用随机数据对组件进行压力测试 软件模拟： 生成随机行为&#x2F;移动。 例如，可能希望玩家&#x2F;敌人以随机模式移动。 可能的方法：随机生成一个 0 到 3 之间的数字、 0 表示向前移动，1 表示向左移动，2 表示向后移动，3 表示向右移动。 地牢的布局可以随机生成 可能希望引入不可预测性 随机数 Random Numbers软件只能生成伪随机数 在java中生成随机数使用 Random 类 需要import java.util.Random 类 选项1：创建一个新的随机数生成器 Random rand &#x3D; new Random(); 选项2：使用单个长种子创建新的随机数生成器。 重要：每次使用相同种子运行程序时，都会得到完全相同的 “随机 “数序列。 Random rand = new Random(long seed)； 为了改变输出，我们可以给随机播种器一个随时间变化的起点。例如，起点（种子）是当前时间 基本测试模板 Basic Test Template 设定先决条件 Precondition (@BeforeEach 等) 实施 (调用方法) 验证后置条件 Postcondition (@AfterEach&#x2F; Asserts 等) 通常情况下，每个测试都应独立运行，执行顺序并不重要 参数化测试 Parameterized Tests 参数化测试 Parameterized Tests 使用不同的输入值反复执行相同的测试，并根据相应的预期结果测试输出 数据源 data source 可用于检索输入值和预期结果的数据。 如果要在每个测试用例之前执行某些语句（如前置条件），可使用 @Before 注解。 如果要在每个测试用例后执行某些语句，如重置变量、删除临时文件和变量等，则可使用 @After 注解。 测试类型 Types of tests单元测试 Unit Test 测试单一功能 理想情况下，应进行隔离测试–采用科学方法，控制所有其他变量 尽量减少对其他功能的依赖 因此，很难编写黑盒单元测试 我们可以通过尽可能减少依赖关系的数量来使我们的测试类似于单元测试 可以使用模拟测试和模拟对象，但这需要了解方法依赖于哪些功能 在不调用另一个方法的情况下，很难说测试单个方法是否发生了变化 集成测试 Integration test 测试依赖关系网（耦合），捕捉单元测试没有发现的 “潜伏在缝隙中 “的错误 每个失败的集成测试都应能写成一个失败的单元测试 测试软件组件之间的交互（耦合） 系统测试 System Test 对整个系统进行黑盒测试 可以在不同的抽象层次进行测试 可用性测试&#x2F;验收测试 Usability tests &#x2F; acceptance tests 测试其在前端是否有效 功能是否达到预期目标 是否可用 基于属性的测试 Property-based test 测试代码的个别属性，而不是直接测试输出 创建测试集换 Creating a test plan需要正确设计测试方法，确保： 高覆盖率 混合不同类型的测试 需要注意的： 编写过多的测试是 不好 的–如果你对同一件事进行单元测试、集成测试和系统测试，那么测试套件就会变得 紧密耦合 ，难以维护。 需要取得 平衡 -&gt;一种方法是用单元测试来测试所有内容，但只用集成测试&#x2F;系统测试来测试程序的主要流程&#x2F;用例–对于项目来说，这将是一个团队决定，并记录在测试计划中。 编写测试代码的原则 Principles of writing test code 就像适用于普通代码的要求同样适用于测试代码，DRY、KISS 可以在测试代码中使用设计模式 不过，在编写测试代码时还需要考虑其他一些事情： 测试代码越简单越好，否则，最终得到的东西会比您首先要测试的软件更复杂（也更容易出错） 应尽量减少条件、循环和任何控制流，以降低测试的复杂性 工厂模式通常在测试设计中非常有用，可以编写一个工厂来生产测试用的假对象 单例模式 Singleton pattern单例是一种创建设计模式，可确保一个类只有一个实例，同时为该实例提供全局访问点 问题客户希望 确保一个类只有一个实例，并 为该实例提供全局访问点 解决方法所有单例的实现都有这两个共同步骤： 将 默认构造函数 设置为私有private，以防止其他对象在单例类中使用 new 操作符。 创建一个作为构造函数的静态创建方法 static creation method。在内部，该方法调用私有构造函数创建对象，并将其保存在静态字段中。接下来对该方法的所有调用都会 返回缓存对象 。 如果你的代码能访问 Singleton 类，那么它就能调用 Singleton 的静态方法。 无论何时调用 Singleton 的静态方法，返回的总是同一个对象。 结构 structure 单例类声明了静态方法 getInstance()，该方法返回其自身类的相同实例 注意：如果程序支持多线程，则需要在 getInstance() 处添加线程锁 synchronized 客户代码应隐藏 Singleton 的构造函数 调用 getInstance() 方法应该是获取单例对象的唯一方法 如何实现 how to implement 在类中添加一个私有静态字段 private static field，用于存储单例实例 声明一个公共静态创建方法 public static creation method，用于获取单例实例 在静态方法中实现 “懒初始化” 首次调用时应创建一个新对象，并将其放入静态字段 该方法应在所有后续调用中始终返回该实例 将类的构造函数设置为私有 constructor of the class private。 类的静态方法仍能调用构造函数，但不能调用其他对象在客户端中，调用单例的静态创建方法来访问对象 样例 Example 并发性入门 Introduction to concurrency 包括 Java 在内的多种现代语言都允许多线程并发执行 为了充分利用当今的多核硬件，我们必须创建采用多线程的应用程序 因此，从根本上了解并发性至关重要 线程安全：多个线程可以访问相同的资源，而不会暴露不正确的行为或导致不可预测的结果 遗憾的是，许多 Java 库都缺乏线程安全。例如，ArrayList、StringBuilde 线程安全 Thread Safety Java 中的时间切分Time slicing是指为线程分配时间的过程 线程运行的顺序是不确定的。一个线程有多少语句在另一个线程的某些语句运行之前运行也是不可预测的 修改同一对象（数据）的两个线程可以并行运行 可行的解决办法：Synchronized 同步方法在开始时获取对象或类的锁，执行方法，然后在结束时释放锁 使用同步关键词只允许一个线程执行方法，避免了并发问题 为了最有效地利用可用的多个 CPU，必须尽量减少访问共享资源的部分代码（在同步下） 我们也可以同步一组语句，但同步一个方法才是好的做法 Java 提供了线程安全的集合封装器，使用静态方法对集合进行线程安全封装。例如，Collections.synchronizedList(list) Java.util.concurrent包 包含适合多线程使用并经过优化的集合。 需要避免的 Need to avoid 当两个或多个线程无限期地互相等待时，这种情况被称为死锁deadlock 当两个或多个线程陷入相互反应的无休止循环时，这种情况被称为活锁livelock 当一个或多个线程因另一个 “贪婪 “的线程而无法继续运行时，就会出现starvation 模板模式 Template pattern 模板方法定义了行为的骨架（结构）（通过实现不变部分invariant parts） 模板方法调用原始操作(primitive operation)，可由子类实现，或在抽象超类中默认实现 子类可以只重新定义行为的某些部分，而不改变其他部分或行为的结构 子类不能控制父类的行为，父类调用子类的操作 关于控制反转 使用库 library（可重用类）时，我们调用想要重用的代码使用框架 framework（如模板模式）时，我们编写子类并实现框架调用的变体代码 模板模式只需实现一次行为的通用（不变）部分，”而由子类来实现可变化的行为 不变行为属于一个类中（localized） 结构 Structure 抽象类定义了一个 templateMethod() 来实现不变结构（行为） templateMethod() 调用抽象类（抽象或具体）中定义的方法–如 primitive1、primitive2 等 可通过提供具体方法在抽象类中实现默认行为 重要的是，子类可以更改原始方法来实现不同的行为(variant behaviour) 子类编写者必须了解哪些操作是为了覆盖而设计的 基本操作Primitive operations：有默认实现或必须由子类实现的操作 最终操作final operations：子类不能重写的具体操作 钩子操作hook operations：默认情况下什么都不做的具体操作，必要时子类可以重新定义。这样，子类就可以根据自己的需要在不同的地方 “挂钩 “算法；子类也可以自由地忽略挂钩 样例 Example 样板模式 vs 策略模式 模板方法在类(class level)一级工作，因此是静态的 策略模式作用于对象层面(object level)，能在运行时切换行为 **模板方法基于继承inheritance**：它允许你通过在子类中扩展算法的某些部分来改变这些部分 策略模式以组合composition为基础：通过在运行时为对象提供与该行为相对应的不同策略，可以改变对象的部分行为 迭代器模式 Iterator pattern迭代器模式是一种行为设计模式， 让你能在不暴露集合底层表现形式 （列表、 栈和树等） 的情况下遍历集合中所有的元素 迭代器 Iterator 迭代器是一种能让程序员遍历容器的对象 允许我们访问数据结构的内容，同时抽象出其底层表示形式 在 Java 中，for 循环是对迭代器的抽象 迭代器可以告诉我们 我们还有剩余的元素吗？ 下一个元素是什么？ 遍历数据结构 Traversing a data structure 聚合实体（容器） 堆栈、队列、列表、树、图、循环 如何遍历聚合实体而不暴露其底层表示？ 保持抽象性和封装性 最初的解决方案–接口中的方法 如果我们需要多种方法来遍历容器，该怎么办？ 解决方法 迭代器模式的主要思想是将集合的遍历行为抽取为单独的迭代器对象 除实现自身算法外， 迭代器还封装了遍历操作的所有细节， 例如当前位置和末尾剩余元素的数量。 因此， 多个迭代器可以在相互独立的情况下同时访问集合。 迭代器通常会提供一个获取集合元素的基本方法。 客户端可不断调用该方法直至它不返回任何内容， 这意味着迭代器已经遍历了所有元素。 所有迭代器必须实现相同的接口。 这样一来， 只要有合适的迭代器， 客户端代码就能兼容任何类型的集合或遍历算法。 如果你需要采用特殊方式来遍历集合， 只需创建一个新的迭代器类即可， 无需对集合或客户端进行修改。 结构 structure 迭代器和可迭代对象 Iterator vs Iterables 可迭代对象是可以被迭代的对象 An iterable is an object that can be iterated over 所有迭代器都是可迭代对象，但并非所有可迭代对象都是迭代器 All iterators are iterable, but not all iterables are iterators For 循环只需给定可遍历对象 访问者模式 Visitor pattern 访问者是模式一种行为设计模式(behavioral design pattern)，它在不修改现有对象的情况下为其添加新的操作&#x2F;行为 访问者设计模式是一种将算法从其操作对象结构中 分离 出来的方法 这是遵循开放&#x2F;封闭原则(open-close principle)的一种方法 创建一个访问者类，实现虚拟操作&#x2F;方法的所有适当特殊化 访问者将实例引用instance reference作为输入，并实现目标（额外的行为） 访问者模式可添加到API接口中，使其客户端无需修改源代码即可对类执行操作 一个应用案例：网购的购物车 + 结算 存在的问题 problem想在不更改底层代码的情况下实现基于底层数据的新功能 解决方法 Solution 访问者模式建议将新行为放入一个名为访问者Visitor的单独类separate class中，而不是试图将其集成到现有类中 必须执行行为的原始对象现在作为参数as an argument传递给访问者的一个方法，使该方法可以访问对象中包含的所有必要数据 访问者类visitor class需要定义一组方法，每种类型一个方法。例如，一个城市、一个景点、一个行业等访问者模式使用一种称为 “双重调度double dispatch“的技术，在给定对象（不同类型）上执行合适的方法 一个对象 “接受 “一个访问者，并告诉它应该执行什么访问方法 一个附加方法允许我们在不进一步修改代码的情况下添加更多行为 结构 Structure Visitor接口声明了一组访问方法，这些方法可以将对象结构的具体元素作为参数 每个 “具体访问者Concrete Visitor“都会针对不同的具体元素类别，实现多个版本的相同行为 元素接口Element interface声明了一种 “接受”访问者的方法。该方法应有一个参数，该参数应与访客接口Visitor Interface的类型一致。 具体元素（Concrete Element）必须实现接收方法。 该方法的目的是根据当前元素类将其调用重定向到相应访问者的方法。请注意，即使元素基类实现了该方法， 所有子类都必须对其进行重写并调用访问者对象中的合适方法。 访问者模式的适用性和限制 Applicability and limitation 适用性在以下情况下，将操作移至访问者类是有益的 需要对对象结构进行许多不相关的操作、 组成对象结构的类是已知的，预计不会改变、 需要经常添加新的操作 算法涉及对象结构的多个类，但希望在一个位置进行管理、 算法需要跨越多个独立的类层次结构 限制 由于新的类通常需要为每个访问者添加新的访问方法，因此类层次结构的扩展会更加困难 适配器模式 Adapter pattern 适配器模式是一种结构型设计模式，它能使接口不兼容的对象能够相互合作 允许将现有类的接口用作另一个接口，适用于客户类 适配器模式通常用于使现有类（API）与客户端类协同工作，而无需修改其源代码 适配器类Adapter class 映射&#x2F;连接两种不同类型&#x2F;接口的功能 适配器模式器为现有的有用类提供了一个包装，使客户类可以使用现有类的功能 适配器模式不提供额外功能 结构 Structure 适配器包含它所封装类的一个实例(adaptee) 适配器会调用封装对象的实例中的方法以达到映射&#x2F;链接功能 样例 Example keep updating "},{"title":"3231note","date":"2023-04-25T14:32:58.000Z","url":"/2023/04/26/3231note/","tags":[["COMP3231","/tags/COMP3231/"]],"categories":[["notes","/categories/notes/"]],"content":"UNSW COMP3231 23T1 notes Lec 02 Processes and ThreadsProcesses:• Also called a task or job • Memory image of an individual program • “Owner” of resources allocated for program execution • Encompasses one or more threads Threads:• Unit of execution 执行命令的单位 • Can be traced • list the sequence of instructions that execute • Belongs to a process • Executes within it. One Process may contain one or more threads. Process TerminationConditions which terminate processes 1. Normal exit (voluntary) 2. Error exit (voluntary) 3. Fatal error (involuntary) 4. Killed by another process (involuntary) Process and Thread StatesRunning Blocked Ready Running → Ready• Voluntary Yield() • End of timeslice Running → Blocked• Waiting for input • File, network, • Waiting for a timer (alarm signal) • Waiting for a resource to become available SchedularSometime called as dispatcher, it will choose a ready process to run. BUT: It is inefficient to search through all processes e.g: Ready Queue &amp; Blocked Queue Thread Model Per process items Per thread items Address spaceGlobal variablesOpen filesChild processesPending alarmsSignals and signal handlersAccounting information Program counterRegistersStack (Each thread has its own stack)State Local variables are per thread (allocated on the stack) Global variables are shared between all threads (Allocated in the data section) Concurrency control is an issue. Dynamically allocated memory can be global or local -&gt; Program defined Thread Usage Model 模型 Characteristics 特性 Threads Parallelism. Blocking system calls Single-threaded Process No parallelism, blocking system calls Finite-state Machine Parallelism, nonblocking system calls, interrupts Summary in Threads Simpler to program than a state machine Less resources associated with threads than multiple complete processes Cheaper to create and destroy Share resources (especially memory) between them Performance: Threads waiting for IO can be overlapped with computing threads If all threads are compute bound(被运算速度限制), then there is no performance improvement (on uniprocessor) Threads can take advantage of the parallelism available on machines with more than one CPU (multiprocessor) Lec 03 Concurrency and SynchronisationConcurrency Example:Trying to modify a global variable in two threads The reason for why there is in-kernel concurrency for single-threaded processes: Multi-tasking Interrupt handling Device drivers System calls Resource sharing Critical Region and how to identifyA critical region is a region of code where shared resources are accessed. We can control access to the shared resource by controlling access to the code that accesses the resource. Uncoordinated entry to the critical region results in a race condition (Concurrency Occurs) HOW TO IDENTIFY: Critical regions are the regions of code that access a shared resource, and correctness relies on the shared resource not being concurrently modified by another thread&#x2F;process&#x2F;entity.\\ HOW TO PREVENT: Critical Regions SolutionsConditions required of any solution to the critical region problem: Mutual exclusion: No two processes&#x2F;threads in the critical region at the same time No assumptions made about speeds or numbers of CPUs No process running outside its critical region may block another process No process waited forever to enter its critical region (Bounded) Mutual exclusion by taking turnsWorks for solving concurrency issues: strict alternation -&gt; each process takes turns Cons (缺点) Busy waiting Process must wait its turn even while the other process is doing something else In a multiple processes condition, must wait for everyone to have a turn Does not guarantee progress if a process no longer needs a turn Poor solution when processes require the critical section at different rates Mutual Exclusion by disabling interruptsHow it works: Before entering a critical region, disable interrupts. After leaving the critical region, enable interrupts. Cons: Only available in the kernel Delays everybody else, even with no contention Slows the interrupt response time Does not work on a multiprocessor Hardware Support for mutual exclusionTest and Set instruction •Can be used to implement lock variables correctly •Load the value of the lock •If lock &#x3D;&#x3D; 0 •Set the lock to 1 •Return the result 0 -&gt; we acquire the lock •if lock &#x3D;&#x3D; 1 • return 1 -&gt; another thread&#x2F;process has the lock •Hardware guarantees that the instruction executes atomically -&gt; not be interrupt Pros: Simple (easy to verify work or not) Available at user level Work with any number of processors To implement any number of lock variables Cons: Busy waits (also named as a spin lock) Consumes CPU Starvation is possible when a process leaves its critical section and more than one process is waiting. Tackling the busy-wait problem:Sleep&#x2F;Wakeup When the process is waiting for an event, it calls sleep to block, instead of busy waiting. The event happens, the event generator (another process) calls wakeup to unblock the sleeping process. Waking a ready&#x2F;running process has no effect. Producer-Consumer Problem(bounded buffer problem)Description: producer produces data items and store items in a buffer. Consumer takes the items out of the buffer and consumes them. Issues:Producer Should sleep when the buffer is full. And wakeup when there is empty space in the buffer. The consumer can call wakeup when it consumes the first entry of the full buffer. Consumer Should sleep when the buffer is empty. And wakeup when there are items available. Producer can call wakeup when it adds the first item to the buffer. SemaphoresTwo primitives that are more powerful than simple sleep and wakeup alone. • P(): proberen, from Dutch to test. • V(): verhogen, from Dutch to increment. • Also called wait &amp; signal, down &amp; up. How it works: If a resource is not available, the corresponding semaphore blocks any process waiting for the resource. Blocked processes are put into a process queue maintained by the semaphore (avoids busy waiting!) When a process releases a resource, it signals this by means of the semaphore. Signaling resumes a blocked process if there is any. Wait (P) and signal (V) operations cannot be interrupted. Complex coordination can be implemented by multiple semaphores. The initial count determines how many waits can progress before blocking and requiring a signal. Producer Consumer problem with semaphores: Summarising SemaphoresSemaphores can be used to solve a variety of concurrency problems. However, programming with then can be error-prone: E.g. must signal for every wait To many or too few signals or waits, or signals and waits in the wrong order, can have bad results. MonitorA higher level synchronisation primitive. Programming language construct IDEA: A set of procedures&#x2F;variables&#x2F;data types are grouped in a special module:monitor. Variables and data types can only be accessed from within the monitor. Only one process&#x2F;thread can be in the monitor at any one time. Mutual exclusion is implemented by the complier. When a thread calls a monitor procedure that has a thread already inside, it will be in the entry queue and will sleep until the current thread exits the monitor. Condition VariableTo allow a process to wait within the monitor, we need to declare a condition variable. Condition variable can only be used with the operations wait and signal. x.wait() Means that the process invoking this operation is suspended until another process invokes. Another thread can enter the monitor while original is suspended. x.signal() this operation resumes only one suspended process. If no process is suspended, then this operation has no effect. How to achieve producer-consumer problem with monitors is in P50 lec03. Synchronisation Primitives in OS&#x2F;161Locks Semaphores Wait (P) &#x2F; signal (V) Condition Variables Note: All three variants must hold the lock passed in. Condition Variables and Bounded Buffers cv_wait() will release the lock before block the process Producer-consumer solution with condition variable Dining Philosophers哲学家就餐问题是一个经典的计算机科学同步问题，由Edsger Dijkstra在1965年提出。问题描述了五位哲学家围坐在圆桌旁的情景，他们在思考问题和吃饭之间交替。圆桌中间有一碗意面，每两位哲学家之间有一根筷子。每位哲学家需要两根筷子才能吃饭。哲学家们不能交谈，只能思考或吃饭。 问题在于设计一个协议，使得每位哲学家都能够在需要时使用两根筷子吃饭，而不会出现死锁（即所有哲学家都在等待筷子，无法继续吃饭）或者饥饿（即某位哲学家长时间无法获得筷子）的情况。 以下是一个可能的解决方案： 我们可以为每根筷子分配一个编号（例如，1到5），并为每位哲学家分配一个编号。每位哲学家在需要吃饭时，先尝试拿起编号较低的筷子，然后尝试拿起编号较高的筷子。当哲学家同时拥有两根筷子时，他们可以开始吃饭。吃完饭后，他们将筷子放回原位，然后继续思考。 这种方法可以避免死锁，因为至少有一位哲学家（具有最低编号筷子的哲学家）可以拿起两根筷子并开始吃饭。其他哲学家则需要等待筷子被放回。虽然这种方法可能导致某些哲学家等待时间较长，但可以确保所有哲学家都有机会吃饭，避免饥饿现象。 Reader and Writer Problem读者-写者问题是一个经典的计算机科学并发控制问题，用于描述多个进程在访问共享资源时需要进行同步的场景。在这个问题中，有一些读者进程和写者进程。读者进程只读取共享资源，而写者进程可以修改共享资源。问题的挑战在于设计一个同步协议，以允许多个读者进程同时访问共享资源，但在写者进程访问资源时，确保其他进程（读者和写者）无法访问资源。 以下是一个可能的解决方案： 我们可以使用互斥量（mutex）和信号量（semaphore）来实现这个协议。在这个解决方案中，我们需要一个互斥量来保护对共享资源的访问计数器，以及一个信号量来控制对共享资源的访问。 读者进程： 请求互斥量以修改访问计数器。 将访问计数器加1。如果这是第一个读者进程，请求信号量以阻止写者进程访问共享资源。 释放互斥量。 访问共享资源。 请求互斥量以修改访问计数器。 将访问计数器减1。如果这是最后一个读者进程，释放信号量以允许写者进程访问共享资源。 释放互斥量。 写者进程： 请求信号量以阻止其他进程访问共享资源。 访问共享资源。 释放信号量以允许其他进程访问共享资源。 这个解决方案允许多个读者进程同时访问共享资源，但当有写者进程需要访问资源时，它们将等待所有当前的读者进程完成。同样，在写者进程访问共享资源时，其他读者和写者进程将被阻止。这种方法可以确保在写者进程访问资源时，没有其他进程可以访问共享资源。 Lec04 DeadLockDeadLock &amp; ResourcesDeadlocks occurs when Processes are granted exclusive access to devices&#x2F;locks&#x2F;tables.. We refer to these entities generally as resources. Deadlock: definition A set of processes is deadlocked if each process in the set is waiting for an event that only another process in the set can cause. In the deadlock situation, no process can Run Release resources Be awakened Four conditions for deadlock Mutual exclusion condition Each resource assigned to 1 process or is available Hold and wait condition Process holding resources can request additional No preemption condition Previously granted resources cannot be forcibly taken away Circular wait condition Must be a circular chain of 2 or more processes Each is waiting for resource held by next member of the chain Deadlock avoidance Just ignore the problem altogether Prevention 预防 Negating on of the four necessary conditions Detection and recovery Dynamic avoidance Careful resource allocation Method 1: Ostrich AlgorithmPretend there is no problem Reasonable if Deadlocks occur very rarely Cost of prevention is high UNIX and Windows takes this approach for some of the more complex resource relationships they manage It’s a trade off between Convenience (engineering approach) Correctness (mathematical approach) Method 2: Deadlock preventionResource allocation rules prevent deadlock by prevent one of the four conditions required for deadlock from occurring: Mutual exclusion Hold and wait No preemption Circular Wait Attacking the Mutual Exclusion Condition (Not feasible)Not feasible in general Some devices&#x2F;resources are intrinsically(本质上) not sharable. Attacking the hold and wait condition (request resources initially)Require processes to request resources before starting A process never has to wait for what it needs Issues: May not know required resources at start of run Which means not always possible Also ties up resources other processes could be using Variations: Process must give up all resources if it would block holding a resource Then request all immediately need Prone(倾向于) to livelock Attacking the No Preemption condition(无优先权条件)(take resources away)Not a viable option Attacking the circular wait condition (Order resources)Numerically ordered resources (Resources ordering is a common technique) Method 3: Detection and recoveryNeed a method to determine if a system is deadlocked. Assuming deadlocked is detected, we need a method of recovery to restore progress to the system. Strategy:Note the resource ownership and requests A cycle can be found within the graph, which denotes a deadlock Detection with Multiple Resources of Each Type Sum of current resource allocation + resources available &#x3D; resources that exist Algorithm Look for an unmarked process Pi, for which the i-th row of R is less than or equal to A. If found, add the i-th row of C to A, and mark Pi, then go back to step 1. If no such process exists, terminate. The remaining processes are deadlocked. Recovery from DeadlockRecovery through preemption • take a resource from some other process • depends on nature of the resource Recovery through rollback • checkpoint a process periodically • use this saved state • restart the process if it is found deadlocked • No guarantee is won’t deadlock again Recovery through killing processes • crudest but simplest way to break a deadlock • kill one of the processes in the deadlock cycle • the other processes get its resources • choose process that can be rerun from the beginning Method 4: Deadlock AvoidanceOnly enough information is available in advance, we can avoid a deadlock. Maximum number of each resource required Safe and Unsafe statesA state is safe if • The system is not deadlocked • There exists a scheduling order that results in every process running to completion, even if they all request their maximum resources immediately Unsafe states are not necessarily deadlocked • With a lucky sequence, all processes may complete • However, we cannot guarantee that they will complete (not deadlock) • Safe states guarantee we will eventually complete all processes •Deadlock avoidance algorithm • Only grant requests that result in safe states LiveLockLivelocked processes are not blocked, change state regularly, but never make progress Bankers AlgorithmModelled on a Banker with Customers • The banker has a limited amount of money to loan customers • Limited number of resources • Each customer can borrow money up to the customer’s credit limit • Maximum number of resources required Basic Idea • Keep the bank in a safe state • So all customers are happy even if they all request to borrow up to their credit limit at the same time. • Customers wishing to borrow such that the bank would enter an unsafe state must wait until somebody else repays their loan such that the the transaction becomes safe StarvationA process never receives the resource it is waiting for, despite the resource (repeatedly) becoming free, the resource is always allocated to another waiting process One Solution: First come, first server Lec05 Processes and Threads ImplementationMIPS RegistersUser-mode accessible registers • 32 general purpose registers • r0 hardwired to zero • r31 the link register for jump-and-link (JAL) instruction HI&#x2F;LO • 2 * 32-bits for multiply and divide PC • Not directly visible • Modified implicitly by jump and branch instructions Branching and JumpingBranching and jumping have a branch delay slot • The instruction following a branch or jump is always executed prior to destination of jump ProcessMemory allocationMinimally consist of three segments • Text • contains the code (instructions) • Data • Global variables • Stack • Activation records of procedure&#x2F;function&#x2F;method • Local variables Note: • data can dynamically grow up • E.g., malloc()-ing • The stack can dynamically grow down • E.g., increasing function call depth or recursion P23 lec05 Mode distinguishUser-mode • Processes (programs) scheduled by the kernel • Isolated from each other • No concurrency issues between each other System-calls transition into and return from the kernel Kernel-mode • Nearly all activities still associated with a process • Kernel memory shared between all processes • Concurrency issues exist between processes concurrently executing in a system call User-level Threads Implementation at user-level • User-level Thread Control Block (TCB), ready queue, blocked queue, and dispatcher • Kernel has no knowledge of the threads (it only sees a single process) • If a thread blocks waiting for a resource held by another thread inside the same process, its state is saved and the dispatcher switches to another ready thread • Thread management (create, exit, yield, wait) are implemented in a runtime support library Pros• Thread management and switching at user level is much faster than doing it in kernel level • No need to trap (take syscall exception) into kernel and back to switch • Dispatcher algorithm can be tuned to the application • E.g. use priorities • Can be implemented on any OS (thread or non-thread aware) • Can easily support massive numbers of threads on a per-application basis • Use normal application virtual memory • Kernel memory more constrained. Difficult to efficiently support wildly differing numbers of threads for different applications Cons• Threads have to yield() manually (no timer interrupt delivery to user level) • Co-operative multithreading • A single poorly design&#x2F;implemented thread can monopolise the available CPU time • There are work-arounds (e.g. a timer signal per second to enable pre-emptive multithreading), they are course grain and a kludge. • Does not take advantage of multiple CPUs (in reality, we still have a single threaded process as far as the kernel is concerned) • If a thread makes a blocking system call (or takes a page fault), the process (and all the internal threads) blocks • Can’t overlap I&#x2F;O with computation Kernel-provided Threads Threads are implemented by the kernel • TCBs (thread control block) are stored in the kernel • A subset of information in a traditional PCB (process control block) • The subset related to execution context • TCBs have a PCB associated with them • Resources associated with the group of threads (the process) • Thread management calls are implemented as system calls • E.g. create, wait, exit Cons• Thread creation and destruction, and blocking and unblocking threads requires kernel entry and exit. • More expensive than user-level equivalent Pros• Preemptive multithreading • Parallelism • Can overlap blocking I&#x2F;O with computation • Can take advantage of a multiprocessor Context SwitchContext Switch is what the lowest level of OS does when an interrupt occurs. A context switch can refer to • A switch between threads • Involving saving and restoring of state associated with a thread • A switch between processes • Involving the above, plus extra state associated with a process. • E.g. memory maps Context Switch OccurrenceA switch between process&#x2F;threads can happen any time the OS is invoked • On a system call • Mandatory if system call blocks or on exit(); • On an exception • Mandatory if offender is killed • On an interrupt • Triggering a dispatch is the main purpose of the timer interrupt A thread switch can happen between any two instructions Note instructions do not equal program statements Context Switch Addition Context switch must be transparent for processes&#x2F;threads • When dispatched again, process&#x2F;thread should not notice that something else was running in the meantime (except for elapsed time) OS must save all state that affects the thread • This state is called the process&#x2F;thread context • Switching between process&#x2F;threads consequently results in a context switch. Lec06 SyscallsSyscall DefinitionCan be viewed as special function calls • Provides for a controlled entry into the kernel • While in kernel, they perform a privileged operation • Returns to original caller with the result The system call interface represents the abstract machine provided by the operating system. CPU Computation ModelThe fetch-execute cycle • Load memory contents from address in program counter (PC) • The instruction • Execute the instruction • Increment PC • Repeat Privileged-mode operationTo protect operating system execution, two or more CPU modes of operation exist • Privileged mode (system-, kernel-mode) • All instructions and registers are available • User-mode • Uses ‘safe’ subset of the instruction set • Only affects the state of the application itself • They cannot be used to uncontrollably interfere with OS • Only ‘safe’ registers are accessible The accessibility of addresses within an address space changes depending on operating mode • To protect kernel code and data • Note: The exact memory ranges are usually configurable, and vary between CPU architectures and&#x2F;or operating systems. System call mechanism securely transfers from user execution to kernel execution and back. System call mechanism overview• Processor mode • Switched from user-mode to kernel-mode • Switched back when returning to user mode • Stack Pointer (SP) • User-level SP is saved and a kernel SP is initialised • User-level SP restored when returning to user-mode • Program Counter (PC) • User-level PC is saved and PC set to kernel entry point • User-level PC restored when returning to user-level • Kernel entry via the designated entry point must be strictly enforced •Registers • Set at user-level to indicate system call type and its arguments • A convention between applications and the kernel • Some registers are preserved at user-level or kernel-level in order to restart user-level execution • Depends on language calling convention etc. • Result of system call placed in registers when returning to user-level • Another convention We need system calls because function calls do not: Change from user to kernel mode&#x2F;back again Restrict possible entry points to secure locations Coprocessor 0 (CP0)The processor control registers are located in CP0 Exception&#x2F;Interrupt management registers Translation management registers Exception management C0_statusWe only focus on 0-15 bits ·c0_causeThe 2nd-6th bit is for ExcCode -&gt; The code number of the exception taken C0_epcThe Exception Program Counter • Points to address of where to restart execution after handling the exception or interrupt Hardware Exception handlingBasic situation: Assume an interrupt occurred as the previous instruction completed &amp; We are in user mode with interrupts enabled. Steps: Instruction address at which to restart after the interrupt is transferred to EPC Interrupts disabled and previous state shifted along What stored in IEc&#x2F;KUc is now in IEp&#x2F;KUp Kernel mode is set, and previous state shifted along What stored in IEp&#x2F;KUp (before 2 a) executed) is now in IEo&#x2F;KUo Code for the exception placed in Cause -&gt; ExcCode in c0_cause is now 0 (0 is the code for interrupt) Address of general exception vector placed in PC CPU now running in kernel mode at the address in PC, with interrupt disabled All information required to Find out what caused the exception Restart after exception handling Is in coprocessor registers (Ignore how the OS handles the exception) Load the contents of EPC Store the EPC back in the PC In the branch delay slot (happens when we jump back to the PC&#x2F;user mode), execute a restore from exception instruction -&gt; move back KU&#x2F;IE Now back in the same state we were when the exception happened MIPS System Calls• System calls are invoked via a syscall instruction. • The syscall instruction causes an exception and transfers control to the general exception handler • A convention (an agreement between the kernel and applications) is required as to how user-level software indicates • Which system call is required • Where its arguments are • Where the result should go OS&#x2F;161 Systems CallsOS&#x2F;161 uses the following conventions: • Arguments are passed and returned via the normal C function calling convention • Additionally • Reg v0 contains the system call number • On return, reg a3 contains • 0: if success, v0 contains successful result • not 0: if failure, v0 has the errno. • v0 stored in errno • -1 returned in v0 Summary of System Call in User Mode• From the caller’s perspective, the read() system call behaves like a normal function call • It preserves the calling convention of the language •However, the actual function implements its own convention by agreement with the kernel • Our OS&#x2F;161 example assumes the kernel preserves appropriate registers(s0-s8, sp, gp, ra). •Most languages have similar libraries that interface with the operating system. System Calls – Kernel Side• Things left to do • Change to kernel stack • Preserve registers by saving to memory (on the kernel stack) • Leave saved registers somewhere accessible to • Read arguments • Store return values • Do the “read()” • Restore registers • Switch back to user stack • Return to application Lec07 Computer Hardware ReviewMemory Hierarchy Cashing•Given two-levels of data storage: small and fast, versus large and slow, • Can speed access to slower storage by using intermediate-speed storage as a cache. CPU cache • CPU cache is fast memory placed between the CPU and main memory • 1 to a few cycles access time compared to RAM access time of tens – hundreds of cycles • Holds recently used data or instructions to save memory accesses. • Matches slow RAM access time to CPU speed if high hit rate • Is hardware maintained and (mostly) transparent to software • Sizes range from few kB to tens of MB. • Usually a hierarchy of caches (2–5 levels), on- and off-chip PerformanceThe performance depends on the hit rate in the first level. Effective Access Time Avoid Waiting for Disk Access• Keep a subset of the disk’s data in main memory OS uses main memory as a cache of disk contents Avoid Waiting for Internet Access• Keep a subset of the Internet’s data on disk Application uses disk as a cache of the internet Lec08 File ManagementFile NamesFile system must provide a convenient naming scheme • Textual Names • May have restrictions • Only certain characters • E.g. no ‘&#x2F;’ characters • Limited length • Only certain format • E.g DOS, 8 + 3 • Case (in)sensitive • Names may obey conventions (.c files for C files) • Interpreted by tools (e.g. UNIX) • Interpreted by operating system (e.g. Windows “con:”) File Structure• Sequence of Bytes • OS considers a file to be unstructured • Applications can impose their own structure • Used by UNIX, Windows, most modern OSes P9 lec08 File Types• Regular files •Directories •Device Files –May be divided into •Character Devices – stream of bytes •Block Devices •Some systems distinguish between regular file types –ASCII text files, binary files File Access Types (Patterns)•Sequential access 顺序读写 –read all bytes&#x2F;records from the beginning –cannot jump around, could rewind or back up –convenient when medium was magnetic tape •Random access 随机读写 –bytes&#x2F;records read in any order –essential for data base systems –read can be … •move file pointer (seek), then read or –lseek(location,…);read(…) •each read specifies the file pointer –read(location,…) Typical File Operations Create Delete Open Close Read Write Append Seek Get attributes Set Attribute Rename Criteria for File OrganizationThings to consider when designing file layout •Rapid access –Needed when accessing a single record –Not needed for batch mode •read from start to finish •Ease of update –File on CD-ROM will not be updated, so this is not a concern •Economy of storage –Should be minimum redundancy in the data –Redundancy can be used to speed access such as an index File Directories•Provide mapping between file names and the files themselves •Contain information about files –Attributes –Location –Ownership •Directory itself is a file owned by the operating system Hierarchical (Tree Structured) Directory•Files can be located by following a path from the root, or master, directory down various branches –This is the absolute pathname for the file •Can have several files with the same file name as long as they have unique path names(同一路径下的文件名不得相等) Relative and Absolute Pathnames•Absolute pathname –A path specified from the root of the file system to the file A Relative pathname –A pathname specified from the cwd (current working directory) Typical Directory Operations Create Delete Opendir Closedir Readdir Rename Link Unlink Nice Properties of UNIX naming•Simple, regular format –Names referring to different servers, objects, etc., have the same syntax. •Regular tools can be used where specialised tools would be otherwise be needed. •Location independent –Objects can be distributed or migrated, and continue with the same names Access RightsIn multiuser system, we need to consider the issue of the access rights. Different kinds of Access Rights: None User may not know the existence of the file User is not allowed to read the directory that includes the file Knowledge User can only determine that the file exists and who its owner is Execution The user can load and execute a program but cannot copy it Reading The user can read the file for any purpose, including copying and execution Appending The user can add data to the file but cannot modify or delete any of the file’s contents Updating The user can modify, delete, and add to the file’s data. This includes creating the file, rewriting it, and removing all or part of the data Changing protection User can change access rights granted to other users Deletion User can delete the file Owners Has all rights previously listed May grant rights to others using the following classes of users •Specific user •User groups •All for public files Simultaneous AccessIn multiuser system, we need to consider the issue of the simultaneous access. •Most OSes provide mechanisms for users to manage concurrent access to files –Example: flock(), lockf(), system calls •Typically –User may lock entire file when it is to be updated –User may lock the individual records (i.e. ranges) during the update •Mutual exclusion and deadlock are issues for shared access Lec09 File System InternalsUNIX Storage StackFrom lower to Higher: Hard disk platters: Tracks Sectors Disk controller Hides disk geometry, bad sectors Exposes linear sequence of blocks(disk’s interface) Device driver Hides device-specific protocol Exposes block-device interface Disk scheduler&#x2F;buffer cache -&gt; Optimisations Keep recently accessed disk blocks in memory Schedule disk accesses from multiple processes for performance and fairness File System Hides physical location of data on the disk Exposes: Directory hierarchy 目录的层次结构 Symbolic file names Random-access files Protrction Virtual File System (VFS) Unified interface to multiple File systems Open File Table (OF table)&#x2F;File Descriptor Table (FD table) Keep track of files opened by user-level processes Matches syscall interface to VFS interface Difference between some FSDifferent physical nature of storage devices – Ext3 is optimised for magnetic disks – JFFS2 is optimised for flash memory devices – ISO9660 is optimised for CDROM Different storage capacities – FAT16 does not support drives &gt;2GB – FAT32 becomes inefficient on drives &gt;32GB – ZFS, Btrfs is designed to scale to multi-TB disk arrays Different CPU and memory requirements – FAT16 is not suitable for modern PCs but is a good fit for many embedded devices Proprietary standards – NTFS may be a nice FS, but its specification is closed Property of hard disk– Seek time • ~15ms worst case – Rotational delay • 8ms worst case for 7200rpm drive – For comparison, disk-to-buffer transfer speed of a modern drive is ~10µs per 4K block Conclusion: keep blocks that are likely to be accessed together close to each other Implementing a file systemRequirementsThe FS must map symbolic file names into a collection of block addresses. The FS must keep track of – which blocks belong to which files. – in what order the blocks form the file – which blocks are free for allocation Given a logical region of a file, the FS must track the corresponding block(s) on disk. File Allocation MethodsContiguous Allocation✔ Easy bookkeeping (need to keep track of the starting block and length of the file) ✔ Increases performance for sequential operations ✗ Need the maximum size for the file at the time of creation ✗ As files are deleted, free space becomes divided into many small chunks (external fragmentation) Example: ISO 9660 (CDROM) Dynamic Allocation Strategies– Disk space allocated in portions as needed – Allocation occurs in fixed-size blocks ✔ No external fragmentation ✔ Does not require pre-allocating disk space ✗ Partially filled blocks (internal fragmentation) ✗ File blocks are scattered across the disk ✗ Complex metadata management (maintain the collection of blocks for each file) Examples come after: Linked list allocation File Allocation Table (FAT) Inode-based FS structure External and Internal fragmentationExternal fragmentation– The space wasted external to the allocated memory regions – Memory space exists to satisfy a request but it is unusable as it is not contiguous Internal fragmentation– The space wasted internal to the allocated memory regions – Allocated memory may be slightly larger than requested memory; this size difference is wasted memory internal to a partition ConclusionInternal fragmentation is inside the allocated memory blocks External fragmentation is between two allocated memory blocks Dynamic allocation : Linked list allocationEach block contains the block number of the next block in the chain. Free blocks are also linked in a chain. ✔ Only single metadata entry per file ✔ Best for sequentially accessed files ✗ Poor for random access ✗ Blocks end up scattered across the disk due to free list eventually being randomised Dynamic Allocation: File allocation Table (FAT)• Keep a map of the entire FS in a separate table – A table entry contains the number of the next block of the file – The last block in a file and empty blocks are marked using reserved values • The table is stored on the disk and is replicated in memory • Random access is fast (following the in-memory list) Issues: Requires a lot of memory for large disks File allocation table disk layout Dynamical Allocation: inode-based FS structureIdea: separate table (index-node or i-node) for each file. – Only keep table for open files in memory – Fast random access The most popular FS structure today Issues: i-nodes occupy one or several disk areas i-nodes are allocated dynamically, hence free-space management is required for i-nodes – Use fixed-size i-nodes to simplify dynamic allocation – Reserve the last i-node entry for a pointer (a block number) to an extension i-node Free-space management Approach 1: Linked list of free blocks in free blocks on disk Keep bitmaps of free blocks and free i-nodes on disk Free block ListList of all unallocated blocks Background jobs can re-order list for better contiguity Store in free blocks themselves – Does not reduce disk capacity Only one block of pointers need be kept in the main memory Bit tablesIndividual bits in a bit vector flags used&#x2F;free blocks 16GB disk with 512-byte blocks –&gt; 4MB table May be too large to hold in main memory Expensive to search – Optimisations possible, e.g. a two level table Concentrating (de)allocations in a portion of the bitmap has desirable effect of concentrating access Simple to find contiguous free space Implementing directories• Directories are stored like normal files – directory entries are contained inside data blocks • The FS assigns special meaning to the content of these files – a directory file is a list of directory entries – a directory entry contains file name, attributes, and the file i-node number • maps human-oriented file name to a system-oriented name Fixed-size directory entries– Either too small • Example: DOS 8+3 characters – Or waste too much space • Example: 255 characters per file name Variable-size directory entries– Freeing variable length entries can create external fragmentation in directory blocks • Can compact when block is in RAM Searching Directory methods Linear scan Implement a directory cache in software to speed-up search Hash lookup B-tree Storing file attributes Disk addresses and attributes in directory entry -&gt; FAT Directory in which each entry just refers to an i-node -&gt; UNIX File system block sizeFile systems deal with 2 types of blocks – Disk blocks or sectors (usually 512 bytes) – File system blocks 512 * 2^N bytes What should N be will be the problem. • Smaller blocks waste less disk space (less internal fragmentation) • Sequential Access – The larger the block size, the fewer I&#x2F;O operations required • Random Access – The larger the block size, the more unrelated data loaded. – Spatial locality of access improves the situation • Choosing an appropriate block size is a compromise Lec10 UNIX File Management (continue)Virtual file system (VFS)Traversing the directory hierarchy may require VFS to issue requests to several underlying file systems. • Provides single system call interface for many file systems – E.g., UFS, Ext2, XFS, DOS, ISO9660,… • Transparent handling of network file systems – E.g., NFS, AFS, CODA • File-based interface to arbitrary device drivers (&#x2F;dev) • File-based interface to kernel data structures (&#x2F;proc) • Provides an indirection layer for system calls – File operation table set up at file open time – Points to actual handling code for particular type – Further file operations redirected to those functions Data Types in VFS InterfaceVFS Represent all file system types Contains pointers to functions to manipulate each file system as a whole Form a standard interface to the file system Vnode Represents a file (inode) in the underlying filesystem Points to the real inode Contains pointers to funcions to manipulate files&#x2F;inodes like open&#x2F;close&#x2F;read File descriptorsAttributes Each open file has a file descriptor Read&#x2F;Write&#x2F;Lseek&#x2F;… use them to specify which file to operate on. State associated with a file descriptor File pointer Determine where in the file the next read or write is performed Mode Was the file opened with read-only permission or sth… Use single fd table with no of tableHow to achieve: Use vnode numbers as file descriptors and add a file pointer to the vnode. Problem: Cannot handling concurrency situations. At this situation: we only have a single global open file array. Entries contains file pointer and a pointer to a vnode. Issues: File descriptor 1 is stdout -&gt; console for some processes but may be a file for other processes Entry 1 should be different different per process! Per-process File descriptor array Each process has its own open file array Issue Fork Fork defines that the child shares the file pointer with the parent Dup2 Also defines the file descriptors share the file pointer With per-process table, we can only have independent file pointers, even when accessing the same file Per-Process fd table with global open file table Per-process file descriptor array contains pointers to open file table entry Open file table array contains entries with a file pointer and a pointer to an vnode. This model provides Shared file pointers if required Independent file pointers if required This model used by linux and most other UNIX operating systems Buffer–Temporary storage used when transferring data between two entities •Especially when the entities work at different rates •Or when the unit of transfer is incompatible •Example: between application program and disk Buffer disk blocks•Allow applications to work with arbitrarily sized region of a file –However, apps can still optimise for a particular block size •Writes can return immediately after copying to kernel buffer –Avoids waiting until write to disk is complete –Write is scheduled in the background •Can implement read-ahead by pre-loading next block on disk into kernel buffer –Avoids having to wait until next read is issued CacheFast storage used to temporarily hold data to speed up repeated access to the data -&gt; such as main memory can cache disk blocks •On access –Before loading block from disk, check if it is in cache first •Avoids disk accesses •Can optimise for repeated access for single or several processes Buffer and caching are related•Data is read into buffer; an extra independent cache copy would be wasteful •After use, block should be cached •Future access may hit cached copy •Cache utilises unused kernel memory space; –may have to shrink, depending on memory demand Cache replacementWhen the buffer cache is full and we need to read another block into memory, we must choose an existing entry to replace Policies: FIFO (first in first out) Least recently used Etc. File system consistencyGenerally, cached disk blocks are prioritised in terms of how critical they are to file system consistency. –Directory blocks, inode blocks if lost can corrupt entire filesystem •These blocks are usually scheduled for immediate write to disk –Data blocks if lost corrupt only the file that they are associated with •These blocks are only scheduled for write back to disk periodically •In UNIX, flushd (flush daemon) flushes all modified blocks to disk every 30 seconds Possible SolutionWrite-through cache All modified blocks are written immediately to disk But this will generate more disk traffic Works in the following situations: Floppies were removed from drives Users were constantly resetting (or crashing) their machines This method is still be used in USB storage. Lec11 ext2 FSDescriptions:• Second Extended Filesystem – The main Linux FS before ext3 – Evolved from Minix filesystem (via “Extended Filesystem”) • Features – Block size (1024, 2048, and 4096) configured at FS creation – inode-based FS – Performance optimisations to improve locality (from BSD FFS) • Main Problem: unclean unmount -&gt; e2fsck – Ext3fs keeps a journal of (meta-data) updates – Journal is a file where updates are logged – Compatible with ext2fs i-node• Each file is represented by an inode on disk • Inode contains the fundamental file metadata – Access rights, owner, accounting info – (partial) block index table of a file • Each inode has a unique number – System oriented name • Directories map file names to inode numbers – Map human-oriented to system-oriented names Inode contents Mode Type Regular file or directory Access mode Rwxrwxrwx Uid User ID Gid Group ID Atime Time of last access Ctime Time when the file was created Mtime Time when file was modified Size Offset of the highest byte written Block count Number of disk blocks used by the file Note that number of blocks can be much less than exected given the file size IMPORTANT: Files can be sparsely populated. Such as using lseek. the blocks between them might be empty. So the size is not the same as the block count. Direct Blocks Block numbers of the first 12 blocks in the file Most files are small We can find block of file directly from the inode. For larger files, we store the data blocks’ offsets greater than 12 blocks in single&#x2F;double&#x2F;triple indirect. Single indirect Requires two disk access to read One for the indirect block, one for the target block. Max file size Assume 1K byte block size, 4 byte block numbers 12 * 1K + 1K&#x2F;4 * 1K &#x3D; 268 KiB So for large majority of files (&lt;268KiB), given the inode, only one or two further accesses required to read any block in file. Double indirect –Block number of a block containing block numbers of blocks containing block numbers -&gt; the second level is also block numbers of blocks, the third level has the block numbers Triple indirect Block number of a block containing block numbers of blocks containing block numbers of blocks containing block numbers Max File SizeAssume 4 bytes block numbers and 1K blocks • The number of addressable blocks – Direct Blocks &#x3D; 12 – Single Indirect Blocks &#x3D; 256 – Double Indirect Blocks &#x3D; 256 * 256 &#x3D; 65536 – Triple Indirect Blocks &#x3D; 256 * 256 * 256 &#x3D; 16777216 • Max File Size 12 + 256 + 65536 + 16777216 &#x3D; 16843020 blocks ≈ 16 GB Example: • Assume 4K blocks, 4 byte block numbers, 12 direct blocks what if we add – lseek(fd, 5242880, SEEK_SET) &#x2F;* 5 megabytes *&#x2F; – write(fd, “x”, 1) Address &#x3D; 5242880 &#x3D;&#x3D;&gt; Block number &#x3D; 5242880&#x2F;4096 &#x3D;1280 Double indirect offset (20-bit) &#x3D; 1280 – 1036 &#x3D; 244 Top 10 bits &#x3D; 0 -&gt; first level block’s 0th entry Lower 10 bits &#x3D; 244-&gt; second level block 244th entry Best and worse case access patternsAssume inode already in memory To read 1 byte – Best: • 1 access via direct block – Worst: • 4 accesses via the triple indirect block To write 1 byte – Best: • 1 write via direct block (with no previous content) – Worst: • 4 reads (to get previous contents of block via triple indirect) + 1 write (to write modified block back) Worst Case Access Patterns with Unallocated Indirect Blocks• Worst to write 1 byte – 4 writes (3 indirect blocks; 1 data) All indirect are new – 1 read, 4 writes (read-write 1 indirect, write 2; write 1 data) First indirect has something – 2 reads, 3 writes (read 1 indirect, read-write 1 indirect, write 1; write 1 data) First and second indirect have something – 3 reads, 2 writes (read 2, read-write 1; write 1 data) All three indirect have something • Worst to read 1 byte – If reading writes a zero-filled block on disk – Worst case is same as write 1 byte – If not, worst-case depends on how deep is the current indirect block tree. Inode SummaryThe inode (and indirect blocks) contains the on-disk metadata associated with a file – Contains mode, owner, and other bookkeeping – Efficient random and sequential access via indexed allocation – Small files (the majority of files) require only a single access – Larger files require progressively more disk accesses for random access • Sequential access is still efficient – Can support really large files via increasing levels of indirection System V Disk Layout (s5fs) – Boot Block • contain code to bootstrap the OS – Super Block • Contains attributes of the file system itself • e.g. size, number of inodes, start block of inode array, start of data block area, free inode list, free data block list – Inode Array – Data blocks Problems• Inodes at start of disk; data blocks end – Long seek times • Must read inode before reading data blocks • Only one superblock – Corrupt the superblock and entire file system is lost • Block allocation was suboptimal – Consecutive free block list created at FS format time • Allocation and de-allocation eventually randomises the list resulting in random allocation • Inode free list also randomised over time – Directory listing resulted in random inode access pattern Berkeley Fast Filesystem (FFS)•Historically followed s5fs –Addressed many limitations with s5fs –ext2fs mostly similar Layout of an Ext2 FS •Partition: –Reserved boot block, –Collection of equally sized block groups –All block groups have the same structure Layout of a block group •Replicated super block –For e2fsck •Replicated Group descriptors •Bitmaps identify used inodes&#x2F;blocks •All block groups have the same number of data blocks •Advantages of this structure: –Replication simplifies recovery –Proximity of inode tables and data blocks (reduces seek time) Superblocks•Size of the file system, block size and similar parameters •Overall free inode and block counters •Data indicating whether file system check is needed: –Uncleanly unmounted –Inconsistency –Certain number of mounts since last check –Certain time expired since last check •Replicated to provide redundancy to aid recoverability Group Descriptors•Location of the bitmaps •Counter for free blocks and inodes in this group •Number of directories in the group •Replicated to provide redundancy to aid recoverability 这些信息有助于文件系统定位和管理每个块组的资源。 Performance considerationsEXT2 optimisations – Block groups cluster related inodes and data blocks –Pre-allocation of blocks on write (up to 8 blocks) •8 bits in bit tables •Better contiguity when there are concurrent writes –Aim to store files within a directory in the same group Ext2 Directories•Directories are files of a special type •Consider it a file of special format, managed by the kernel, that uses most of the same machinery to implement it –Inodes, etc… •Directories translate names to inode numbers •Directory entries are of variable length •Entries can be deleted in place •inode &#x3D; 0 •Add to length of previous entry “f1” &#x3D; inode 7 “file2” &#x3D; inode 43 “f3” &#x3D; inode 85 Hard LinksInodes can have more than one name -&gt; hard link Reference Count in inodeKeep a reference count in the inode, so: When we delete a file by name, i.e. remove the directory entry (link), the file system know when to delete the underlying inode •Adding a name (directory entry) increments the count •Removing a name decrements the count •If the reference count &#x3D;&#x3D; 0, then we have no names for the inode (it is unreachable), we can delete the inode (underlying file or directory) Symbolic links• A symbolic link is a file that contains a reference to another file or directory – Has its own inode and data block, which contains a path to the target file – Marked by a special file attribute – Transparent for some operations – Can point across FS boundaries -&gt; symbolic link only care about the path of the certain file, instead of the destination. File system reliability• Disk writes are buffered in RAM – OS crash or power outage &#x3D;&#x3D;&gt; lost data – Commit writes to disk periodically (e.g., every 30 sec) – Use the sync command to force a FS flush • FS operations are non-atomic – Incomplete transaction can leave the FS in an inconsistent state Other examples• e2fsck – Scans the disk after an unclean shutdown and attempts to restore FS invariants • Journaling file systems – Keep a journal of FS updates – Before performing an atomic update sequence, – write it to the journal – Replay the last journal entries upon an unclean shutdown – Example: ext3fs Lec12 ext3 FSDesign goals– Add journaling capability to the ext2 FS – Backward and forward compatibility with ext2 • Existing ext2 partitions can be mounted as ext3 – Leverage the proven ext2 performance – Reuse most of the ext2 code base – Reuse ext2 tools, including e2fsck The ext3 journalOption 1: Journal FS data structure updatesExample: – Start transaction – Delete dir entry – Delete i-node – Release blocks 32, 17, 60 – End transaction Conclusion✔Efficient use of journal space; hence faster journaling ✘ Individual updates are applied separately ✘ The journaling layer must understand FS semantics Option 2: Journal disk block updatesExample: – Start transaction – Update block #n1 (contains the dir entry) – Update block #n2 (i-node allocation bitmap) – Update block #n3 (data block allocation bitmap) – Add transaction Conclusion✗ Even a small update adds a whole block to the journal ✔ Multiple updates to the same block can be aggregated into a single update ✔ The journaling layer is FS independent (easier to implement) Ext3 use this method Journaling Block Device (JBD) JBD interface – Start a new transaction – Update a disk block as part of a transaction – Complete a transaction • Completed transactions are buffered in RAM – Commit: write transaction data to the journal (persistent storage) • Multiple FS transactions are committed in one go – Checkpoint: flush the journal to the disk • Used when the journal is full or the FS is being unmounted Journaling modesExt3 supports two journaling modes – Metadata+data • Enforces atomicity of all FS operations – Metadata journaling • Metadata is journalled • Data blocks are written directly to the disk • Improves performance • Enforces file system integrity • Does not enforce atomicity of write’s – New file content can be stale blocks JBD conclusionJBD • JBD can keep the journal on a block device or in a file – Enables compatibility with ext2 (the journal is just a normal file) • JBD is independent of ext3-specific data structures – Separation of concerns • The FS maintains on-disk data and metadata • JBD takes care of journaling – Code reuse • JBD can be used by any other FS that requires journaling Lec13 memory ManagementFixed Partition Summary• Simple • Easy to implement •Can result in poor memory utilisation • Due to internal fragmentation •Used on IBM System 360 operating system (OS&#x2F;MFT) • Announced 6 April, 1964 • Still applicable for simple embedded systems • Static workload known in advance Dynamic PartitioningAllocation•Also applicable to malloc()-like in-application allocators •Given a region of memory, basic requirements are: • Quickly locate a free partition satisfying the request • Minimise CPU time search • Minimise external fragmentation • Minimise memory overhead of bookkeeping • Efficiently support merging two adjacent free partitions into a larger partition Classic ApproachRepresent available memory as a linked list of available “holes” (free memory ranges). • Base, size • Kept in order of increasing address • Simplifies merging of adjacent holes into larger holes. • List nodes be stored in the “holes” themselves PlacementFirst-fit algorithm• Scan the list for the first entry that fits • If greater in size, break it into an allocated and free part • Intent: Minimise amount of searching performed • Aims to find a match quickly • Biases allocation to one end of memory • Tends to preserve larger blocks at the end of memory Next-fitLike first-fit, except it begins its search from the point in list where the last request succeeded instead of at the beginning. • (Flawed) Intuition: spread allocation more uniformly over entire memory to avoid skipping over small holes at start of memory • Performs worse than first-fit as it breaks up the large free space at end of memory Best-fit• Chooses the block that is closest in size to the request • Performs worse than first-fit • Has to search complete list • does more work than first-fit • Since smallest block is chosen for a process, the smallest amount of external fragmentation is left • Create lots of unusable holes Worst-fit• Chooses the block that is largest in size (worst-fit) • (whimsical) idea is to leave a usable fragment left over • Poor performer • Has to do more work (like best fit) to search complete list • Does not result in significantly less fragmentation SummaryFirst-fit generally better than the others and easiest to implement They are simple solutions to a still-existing OS or application service&#x2F;function –memory allocation. Largely have been superseded by more complex and specific allocation strategies CompactionWe can reduce external fragmentation by compaction • Shuffle memory contents to place all free memory together in one large block. • Only if we can relocate running programs? • Pointers? • Generally requires hardware support Issues• Relocation • How does a process run in different locations in memory? • Protection • How do we prevent processes interfering with each other When are memory address bound?• Compile&#x2F;link time • Compiler&#x2F;Linker binds the addresses • Must know “run” location at compile time • Recompile if location changes • Load time • Compiler generates relocatable code • Loader binds the addresses at load time • Run time • Logical compile-time addresses translated to physical addresses by special hardware. Hardware Support for Runtime Binding and Protection• For process to run using logical addresses • Process expects to access addresses from zero to some limit of memory size • Need to add an appropriate offset to its logical addresses • Achieve relocation • Protect memory “lower” than the process • Must limit the maximum logical address the process can generate • Protect memory “higher” than the process Base and Limit Registers•Also called • Base and bound registers • Relocation and limit registers •Base and limit registers • Restrict and relocate the currently active process • Base and limit registers must be changed at • Load time • Relocation (compaction time) • On a context switch Pros• Supports protected multi-processing (-tasking) Cons• Physical memory allocation must still be contiguous • The entire process must be in memory • Do not support partial sharing of address spaces • No shared code, libraries, or data structures between processes Swapping• A process can be swapped temporarily out of memory to a backing store, and then brought back into memory for continued execution. • Swapping involves transferring the whole process • Backing store – fast disk large enough to accommodate copies of all memory images for all users; must provide direct access to these memory images. • Can prioritize – lower-priority process is swapped out so higher-priority process can be loaded and executed. • Major part of swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped. • slow Virtual memory-paging• Partition physical memory into small equal sized chunks • Called frames • Divide each process’s virtual (logical) address space into same size chunks • Called pages • Virtual memory addresses consist of a page number and offset within the page • OS maintains a page table • contains the frame location for each page • Used by hardware to translate each virtual address to physical address • The relation between virtual addresses and physical memory addresses is given by page table • Process’s physical memory does not have to be contiguous Paging• No external fragmentation • Small internal fragmentation (in last page) • Allows sharing by mapping several pages to the same frame • Abstracts physical organisation • Programmer only deal with virtual addresses • Minimal support for logical organisation • Each unit is one or more pages Memory Management Unit (Translation Look-aside Buffer&#x2F;TLB) Lec14 Virtual memoryPage-based VMVirtual Memory – Divided into equal sized pages – A mapping is a translation between • A page and a frame • A page and invalid – Mappings defined at runtime • They can change – Address space can have holes – Process does not have to be contiguous in physical memory Typical Address space Layout• Stack region is at top, and can grow down • Heap has free space to grow up • Text is typically read-only • Kernel is in a reserved, protected, shared region • 0-th page typically not used, why? -&gt; NULL pointer • A process may be only partially resident – Allows OS to store individual pages on disk – Saves memory for infrequently used data &amp; code • What happens if we access non resident memory? -&gt; Page fault Page fault• Referencing an invalid page triggers a page fault • An exception handled by the OS • Broadly, two standard page fault types – Illegal Address (protection error) • Signal or kill the process – Page not resident • Get an empty frame • Load page from disk • Update page (translation) table (enter frame #, set valid bit, etc. • Restart the faulting instruction Shared PagesPrivate code and data– Each process has own copy of code and data – Code and data can appear anywhere in the address space Shared code– Single copy of code shared between all processes executing it – Code must not be self modifying – Code must appear at same address in all processes Trashing• CPU utilisation tends to increase with the degree of multiprogramming – number of processes in system • Higher degrees of multiprogramming – less memory available per process • Some process’s working sets may no longer fit in RAM – Implies an increasing page fault rate • Eventually many processes have insufficient memory – Can’t always find a runnable process – Decreasing CPU utilisation – System become I&#x2F;O limited This is trashing Recovery From ThrashingIn the presence of increasing page fault frequency and decreasing CPU utilisation – Suspend a few processes to reduce degree of multiprogramming – Resident pages of suspended processes will migrate to backing store – More physical memory becomes available • Less faults, faster progress for runnable processes – Resume suspended processes later when memory pressure eases Page SizeIncreasing page size Increases internal fragmentation § reduces adaptability to working set size Decreases number of pages Reduces size of page tables Increases TLB coverage Reduces number of TLB misses Increases page fault latency Need to read more from disk before restarting process Increases swapping I&#x2F;O throughput Small I&#x2F;O are dominated by seek&#x2F;rotation delays Optimal page size is a (work-load dependent) trade-off Lec16 Multiprocessor SystemsAmdahl’s law•Given a proportion P of a program that can be made parallel, and the remaining serial portion (1-P), speedup by using N processor Bus-based uniform memory access multiprocessorsSimplest MP is more than one processor on a single bus connect to memory • Access to all memory occurs at the same speed for all processors. • Bus bandwidth becomes a bottleneck with more than just a few CPUs Multiprocessor cachesEach processor has a cache to reduce its need for access to memory • Hope is most accesses are to the local cache • Bus bandwidth still becomes a bottleneck with many CPUs Cache consistencyCache consistency is usually handled by the hardware. • Writes to one cache propagate to, or invalidate appropriate entries on other caches • Cache transactions also consume bus bandwidth Multi-core Processor SummaryWith only a single shared bus, scalability can be limited by the bus bandwidth of the single bus • Caching only helps so much •Multiprocessors can • Increase computation power beyond that available from a single CPU • Share resources such as disk and memory •However • Assumes parallelizable workload to be effective • Assumes not I&#x2F;O bound • Shared buses (bus bandwidth) limits scalability • Can be reduced via hardware design • Can be reduced by carefully crafted software behaviour • Good cache locality together with limited data sharing where possible Each CPU has its own OS• Statically allocate physical memory to each CPU • Each CPU runs its own independent OS • Share peripherals • Each CPU (OS) handles its processes system calls • Used in early multiprocessor systems to ‘get them going’ • Simpler to implement • Avoids CPU-based concurrency issues by not sharing • Scales – no shared serial sections • Modern analogy, virtualisation in the cloud Issues• Each processor has its own scheduling queue • We can have one processor overloaded, and the rest idle • Each processor has its own memory partition • We can a one processor thrashing, and the others with free memory • No way to move free memory from one OS to another Symmetric Multiprocessors (SMP) • OS kernel run on all processors • Load and resource are balance between all processors • Including kernel execution • Issue: Real concurrency in the kernel • Need carefully applied synchronisation primitives to avoid disaster • One alternative: A single mutex that make the entire kernel a large critical section • Only one CPU can be in the kernel at a time • The “big lock” becomes a bottleneck when in-kernel processing exceeds what can be done on a single CPU • Better alternative: identify largely independent parts of the kernel and make each of them their own critical section • Allows more parallelism in the kernel • Issue: Difficult task • Code is mostly similar to uniprocessor code • Hard part is identifying independent parts that don’t interfere with each other • Remember all the inter-dependencies between OS subsystems. Multiprocessor synchronisationGiven we need synchronisation, how can we achieve it on a multiprocessor machine? • Unlike a uniprocessor, disabling interrupts does not work. • It does not prevent other CPUs from running in parallel • Need special hardware support Test and setHardware guarantees that the instruction executes atomically on a CPU. • Atomically: As an indivisible unit. • The instruction can not stop half way through • It does not work without some extra hardware support • A solution:• Hardware blocks all other CPUs from accessing the bus during the TSL instruction to prevent memory accesses by any other CPU. • TSL has mutually exclusive access to memory for duration of instruction BUT Test-and Set is a busy-wait synchronisation primitive. -&gt; Spinlock • Issue:• Lock contention leads to spinning on the lock • Spinning on a lock requires blocking the bus which slows all other CPUs down • Independent of whether other CPUs need a lock or not • Causes bus contention Caching does not help reduce bus contention• Either TSL still blocks the bus • Or TSL requires exclusive access to an entry in the local cache • Requires invalidation of same entry in other caches, and loading entry into local cache • Many CPUs performing TSL simply bounce a single exclusive entry between all caches using the bus Reducing Bus contention• Read before TSL • Spin reading the lock variable waiting for it to change • When it does, use TSL to acquire the lock • Allows lock to be shared read-only in all caches until its released • no bus traffic until actual release • No race conditions, as acquisition is still with TSL. Benchmark • Test and set performs poorly once there is enough CPUs to cause contention(争夺) for lock • Expected • Read before Test and Set performs better • Performance less than expected • Still significant contention on lock when CPUs notice release and all attempt acquisition • Critical section performance degenerates • Critical section requires bus traffic to modify shared structure • Lock holder competes with CPU that’s waiting as they test and set, so the lock holder is slower • Slower lock holder results in more contention Spinning vs blocking and switchingUniprocessor• Spinning (busy-waiting) on a lock makes no sense on a uniprocessor • The was no other running process to release the lock • Blocking and (eventually) switching to the lock holder is the only sensible option. •On SMP systems, the decision to spin or block is not as clear. • The lock is held by another running CPU and will be freed without necessarily switching away from the requestor Multiprocessor• Blocking and switching • to another process takes time • Save context and restore another • Cache contains current process not new process • Adjusting the cache working set also takes time • TLB is similar to cache • Switching back when the lock is free encounters the same again • Spinning wastes CPU time directly Trade off• If lock is held for less time than the overhead of switching to and back ÞIt’s more efficient to spin Spinlocks expect critical sections to be short No waiting for I&#x2F;O within a spinlock No nesting locks within a spinlock Preemption and spinlocks• Critical sections synchronised via spinlocks are expected to be short • Avoid other CPUs wasting cycles spinning • What happens if the spinlock holder is preempted at end of holder’s timeslice • Mutual exclusion is still guaranteed • Other CPUs will spin until the holder is scheduled again!!!!! Spinlock implementations disable interrupts in addition to acquiring locks • avoids lock-holder preemption • avoids spinning on a uniprocessor Lec17 schedulingBasicThe scheduler decides who to run next. – The process of choosing is called scheduling. Scheduling decisions can have a dramatic effect on the perceived performance of the system – Can also affect correctness of a system with deadlines Application Behaviour CPU-Bound process • Spends most of its computing • Time to completion largely determined by received CPU time I&#x2F;O-Bound process – Spend most of its time waiting for I&#x2F;O to complete • Small bursts of CPU to process I&#x2F;O and request next I&#x2F;O – Time to completion largely determined by I&#x2F;O request time Observation• We need a mix of CPU-bound and I&#x2F;O-bound processes to keep both CPU and I&#x2F;O systems busy • Process can go from CPU- to I&#x2F;O-bound (or vice ersa) in different phase of execution Insight• Choosing to run an I&#x2F;O-bound process delays a CPU-bound process by very little • Choosing to run a CPU-bound process prior to an I&#x2F;O-bound process delays the next I&#x2F;O request significantly – No overlap of I&#x2F;O waiting with computation – Results in device (disk) not as busy as possible Þ Generally, favour I&#x2F;O-bound processes over CPU-bound processes When is scheduling performedwhen a process (or thread) can no longer continue, or when an activity results in more than one ready process. Preemptive versus Non-preemptive Scheduling 先发调度与后发调度Non-preemptive– Once a thread is in the running state, it continues until it completes, blocks on I&#x2F;O, or voluntarily yields the CPU – A single process can monopolised the entire system Preemptive– Current thread can be interrupted by OS and moved to ready state. – Usually after a timer interrupt and process has exceeded its maximum run time • Can also be as a result of higher priority process that has become ready (after I&#x2F;O interrupt). – Ensures fairer service as single thread can’t monopolise the system • Requires a timer interrupt Categories of scheduling algorithms• The choice of scheduling algorithm depends on the goals of the application (or the operating system) – No one algorithm suits all environments • We can roughly categorise scheduling algorithms as follows – Batch Systems • No users directly waiting, can optimise for overall machine performance – Interactive Systems • Users directly waiting for their results, can optimise for users perceived performance – Realtime Systems • Jobs have deadlines, must schedule such that all jobs (predictably) meet their deadlines. Goals of scheduling AlgorithmsAll algorithms– Fairness • Give each process a fair share of the CPU – Policy Enforcement • What ever policy chosen, the scheduler should ensure it is carried out – Balance&#x2F;Efficiency • Try to keep all parts of the system busy Interactive Algorithms– Minimise response time • Response time is the time difference between issuing a command and getting the result – E.g selecting a menu, and getting the result of that selection • Response time is important to the user’s perception of the performance of the system. – Provide Proportionality • Proportionality is the user expectation that short jobs will have a short response time, and long jobs can have a long response time. • Generally, favour short jobs Real-time Algorithms– Must meet deadlines • Each job&#x2F;task has a deadline. • A missed deadline can result in data loss or catastrophic failure – Aircraft control system missed deadline to apply brakes – Provide Predictability • For some apps, an occasional missed deadline is okay – E.g. DVD decoder • Predictable behaviour allows smooth DVD Round Robin Scheduling• Each process is given a timeslice to run in • When the timeslice expires, the next process preempts the current process, and runs for its timeslice, and so on – The preempted process is placed at the end of the queue • Implemented with – A ready queue – A regular timer interrupt Pros – Fair, easy to implement Cons– Assumes everybody is equal Issue: how long should the timeslice be– Too short • Waste a lot of time switching between processes • Example: timeslice of 4ms with 1ms context switch &#x3D; 20% round robin overhead – Too long • System is not responsive • Example: timeslice of 100ms – If 10 people hit “enter” key simultaneously, the last guy to run will only see progress after 1 second. • Degenerates into FCFS if timeslice longer than burst length Priorities• Each Process (or thread) is associated with a priority • Provides basic mechanism to influence a scheduler decision: – Scheduler will always chooses a thread of higher priority over lower priority • Priorities can be defined internally or externally – Internal: e.g. I&#x2F;O bound or CPU bound – External: e.g. based on importance to the user Priority-driven preemptively scheduled Usually implemented by multiple priority queues, with round robin on each queue Con:– Low priorities can starve • Need to adapt priorities periodically – Based on ageing or execution history Traditional UNIX scheduler• Two-level scheduler: – High-level scheduler schedules processes between memory and disk – Low-level scheduler is CPU scheduler • Based on a multi-level queue structure with round robin at each level • The highest priority (lower number) is scheduled • Priorities are re-calculated once per second, and re-inserted in appropriate queue – Avoid starvation of low priority threads – Penalise CPU-bound threads • Priority &#x3D; CPU_usage +nice +base – CPU_usage &#x3D; number of clock ticks • Decays over time to avoid permanently penalising the process – Nice is a value given to the process by a user to permanently boost or reduce its priority • Reduce priority of background jobs – Base is a set of hardwired, negative values used to boost priority of I&#x2F;O bound system activities • Swapper, disk I&#x2F;O, Character I&#x2F;O Multiprocessor schedulingA single shared ready queueWhen a CPU goes idle, it takes the highest priority process from the shared ready queue Pros– Simple – Automatic load balancing Cons– Lock contention on the ready queue can be a major bottleneck • Due to frequent scheduling or many CPUs or both – Not all CPUs are equal • The last CPU a process ran on is likely to have more related entries in the cache -&gt; waste time to load another process Affinity schedulingBasic Idea – Try hard to run a process on the CPU it ran on last time • One approach: Multiple Queue Multiprocessor Scheduling Multiple Queue SMP Scheduling• Each CPU has its own ready queue • Coarse-grained algorithm assigns processes to CPUs – Defines their affinity, and roughly balances the load • The bottom-level fine-grained scheduler: – Is the frequently invoked scheduler (e.g. on blocking on I&#x2F;O, a lock, or exhausting a timeslice) – Runs on each CPU and selects from its own ready queue • Ensures affinity – If nothing is available from the local ready queue, it runs a process from another CPUs ready queue rather than go idle • Termed “Work stealing ProsNo lock contention on per-CPU ready queues in the (hopefully) common case – Load balancing to avoid idle queues – Automatic affinity to a single CPU for more cache friendly behaviour 多队列 SMP 调度（Multiple Queue SMP Scheduling）是一种用于多处理器（SMP，Symmetric Multi-Processing）系统的进程调度策略。在这种策略中，每个处理器都有自己的一组独立的进程队列。这种方法旨在通过将进程分配到不同的处理器上，实现更好的负载平衡和并行性。 多队列 SMP 调度的主要特点如下： 独立的进程队列：在多队列 SMP 调度中，每个处理器都有自己的一组独立的进程队列，用于存储和管理等待执行的进程。每个处理器根据自己的队列中的进程来执行调度和分配任务。 负载平衡：多队列 SMP 调度策略尝试在处理器之间实现负载平衡。当一个处理器的队列中的进程数量明显多于其他处理器时，系统可能会将一些进程迁移到其他处理器的队列中，以实现更好的负载平衡。 并行性：多队列 SMP 调度充分利用了多处理器系统的并行性。通过在不同的处理器上同时执行多个进程，可以提高系统的整体性能和吞吐量。 局部性：由于每个处理器都有自己的进程队列，多队列 SMP 调度有助于维护进程的局部性。这意味着一个进程可能会在同一个处理器上连续执行，从而提高缓存命中率和性能。 然而，多队列 SMP 调度也存在一些挑战和缺点，例如在负载平衡和同步开销方面的问题。为了解决这些问题，研究人员和工程师们还提出了许多其他的多处理器调度策略，如全局队列调度、簇调度等。 总之，多队列 SMP 调度是一种适用于多处理器系统的进程调度策略，通过为每个处理器分配独立的进程队列来实现负载平衡和并行性。尽管存在一些挑战，但这种方法仍然是多处理器调度的一个重要方案 Cons: 负载平衡困难：尽管多队列 SMP 调度试图在处理器之间实现负载平衡，但在实践中，达到理想的负载平衡可能非常具有挑战性。在某些情况下，可能会出现一个或多个处理器的队列拥有大量进程，而其他处理器却处于空闲状态。这种不平衡可能导致系统资源的浪费和性能下降。 同步和通信开销：在多队列 SMP 调度中，处理器之间可能需要频繁地进行进程迁移以实现负载平衡。这会导致同步和通信开销增加，从而降低系统性能。在具有许多处理器的系统中，同步和通信开销可能会变得非常显著。 进程优先级管理困难：由于每个处理器都有自己的进程队列，管理不同处理器队列中的进程优先级可能变得复杂。在全局队列调度策略中，所有处理器共享一个进程队列，因此优先级管理相对更简单。 高速缓存一致性问题：多队列 SMP 调度中的进程迁移可能会导致高速缓存一致性问题。当一个进程从一个处理器迁移到另一个处理器时，它可能需要重新填充高速缓存，这会导致性能下降。虽然多队列 SMP 调度有助于维护进程的局部性，但频繁的进程迁移可能会削弱这一优势。 可伸缩性问题：在具有大量处理器的系统中，多队列 SMP 调度可能会遇到可伸缩性问题。随着处理器数量的增加，实现有效的负载平衡、优先级管理和同步变得更加困难。 尽管存在这些弊端，多队列 SMP 调度仍然是多处理器系统中的一种有效调度策略。为了解决这些问题，研究人员和工程师们还提出了许多其他的多处理器调度策略，如全局队列调度、簇调度等。这些策略在不同方面具有各自的优缺点，因此需要根据具体应用场景和需求进行选择。 TO BE NOTICED: NOT CONTAIN PART OF LEC 14 AND MOST PART OF LEC 15"},{"title":"test for categories","date":"2023-03-21T08:13:40.000Z","url":"/2023/03/21/test-for-cat/","tags":[["test","/tags/test/"]],"categories":[["Demo","/categories/Demo/"]],"content":"This is a blog for testing! 仅供测试左下角有音乐哦玩得愉快 JUST A DEMO! test for alerts喵呼呼喵呼呼o(&#x3D;•ェ•&#x3D;)m 成功成功啦o(￣▽￣)ブ 危险有危险Σ(っ °Д °;)っ 消息有消息(・∀・(・∀・(・∀・*) 当心当心哦≧ ﹏ ≦ test for folder 这是一个展开了的folder 什么都没有捏 这是一个关闭的folder 记得打开音乐哦！ *你上当了捏* test for blur这是一句被模糊的语句 test for image insert尝试点击一下吧👇 test for playing youtube 没想到吧是双重诈骗 thats all！"},{"title":"hello-world","date":"2023-03-20T06:04:12.000Z","url":"/2023/03/20/hello-world/","categories":[["Demo","/categories/Demo/"]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"}]